[
  {
    "objectID": "about.html#relevant-links",
    "href": "about.html#relevant-links",
    "title": "About Me",
    "section": "Relevant Links",
    "text": "Relevant Links\n\nMy Google Scholar has an up to date set of my publications and preprints.\nMy YouTube Channel contains lecture and tutorial videos from courses I have taught, illustrating my teaching style.\nMy CV is available in PDF form (last updated October, 2022).\nMy Teaching Philosophy Statement has been published as a blog post.\nMy Research Philosophy Statement has been published as a blog post."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "About Me",
    "text": "About Me\nMy postdoctoral research focuses on areas of causal inference related to respondent-driven sampling (RDS). RDS is a survey sampling technique employed to study hard-to-reach population. The sample is formed by starting from identified members of the target population, and using their personal networks to recruit further members of the target population in a way that is probabalistically motivated. There exists comparatively little methodology for analyzing data from RDS, though this form of data collection is becoming increasingly common. My postdoctoral research looks to expand on this existing work, moving beyond the estimation of means and proportions, allowing for associational and causal modelling that is expected when employing other methods of sampling. Moreover, my work seeks to expand these techniques to longitudinal RDS data, a question which has to date received no research attention.\nAlongside these questions, I will continue to pursue methodologies related to dynamic treatment regimes, the core topic studied during my graduate schooling. During my graduate studies, my research focused on measurement error and causal inference. Briefly, measurement error occurs whenever we are interested in measuring something and we do a bad job of it. This happens in almost every study that is run, and unfortunately means that the conclusions that we draw may not be accurate: statistical work on measurement error tries to correct this. Causal inference asks questions of the form “Does X cause Y?” [For instance “Does smoking cause lung cancer?” (yes, it does).] I have a keen interest in providing a theoretical basis for (comparatively) straightforward methods, which are easy to use for non-statisticians, while exhibiting provably good theoretical properties.\nOutside of causal inference and measurement error, I am interested in machine learning, and in particular in trying to establish a statistical basis for novel machine learning techniques (including questions related to inference, interpretability, and model selection).\nI previously did an undergraduate degree in Finance and Mathematics at Queen’s University (I transferred there after completing my first year at Waterloo/Laurier in the ‘Double Degree’ program), and a Master’s of Statistics at Waterloo.\nOutside of my research, I pay very close attention to sports, mostly hockey, (and how statistics is, or should be, applied there), play music (without any connection to statistics), and enjoy board/video games (with varying degrees of statistical relevance). I have a cat (Charles) who is wonderful."
  },
  {
    "objectID": "about.html#my-teaching",
    "href": "about.html#my-teaching",
    "title": "About Me",
    "section": "My Teaching",
    "text": "My Teaching\n\nIn the Winter of 2022 I taught STAT 437 at the University of Waterloo. The course was offered online, and the material is available on my website.\nIn the Winter of 2021 I was one of the instructors for STAT 231 at the University of Waterloo. As a part of my role for this I produced a set of weekly tutorial videos. These are available to view in a YouTube playlist .\nI have served as a teaching assistant for 18 courses between my time at the University of Waterloo and Queen’s University. A full list is available in my CV."
  },
  {
    "objectID": "about.html#contacting-me",
    "href": "about.html#contacting-me",
    "title": "About Me",
    "section": "Contacting Me",
    "text": "Contacting Me\n\nEmail: See my departmental website.\nTwitter: @DylanSpicker. (As of November 2021, I am incredibly inactive).\nLinkedIn: In case anyone is still using this."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Thoughts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\njob search\n\n\n\n\nThis document outlines my research experience, approach, and goals for the future.\n\n\n\n\n\n\nNov 14, 2021\n\n\nDylan Spicker\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\njob search\n\n\n\n\nThis document outlines my teaching experience, approach, and goals for the future.\n\n\n\n\n\n\nNov 14, 2021\n\n\nDylan Spicker\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nI am attempting to work through Luisa Fernholz’s excellent ‘von Mises Calculus for Statistical Functionals’ – these are my notes, pulling in the required concepts from elsewhere..\n\n\n\n\n\n\nApr 27, 2021\n\n\nDylan Spicker\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nThe idea of null preservation is an interesting, and under-discussed, concept.\n\n\n\n\n\n\nJul 16, 2019\n\n\nDylan Spicker\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nresearch\n\n\n\n\nThis blog post explores when, if ever, the Q-functions (from DTRs) can be linear.\n\n\n\n\n\n\nNov 15, 2019\n\n\nDylan Spicker\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nThis document investigates the all-too-familiar claim: ‘Correlation is not causation’.\n\n\n\n\n\n\nMay 26, 2019\n\n\nDylan Spicker\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nmisconceptions\n\n\n\n\nThere is a commonly taught ‘rule-of-thumb’ that states that we can determine the skewness of a distribution based on the relative location of the mean and the median. This is not true.\n\n\n\n\n\n\nApr 20, 2021\n\n\nDylan Spicker\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nWe are often told to control for confounding factors, but when should we?\n\n\n\n\n\n\nJul 1, 2019\n\n\nDylan Spicker\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/STAT231/course_index.html",
    "href": "courses/STAT231/course_index.html",
    "title": "STAT 231",
    "section": "",
    "text": "Week 002: This tutorial goes over the concepts from weeks one and two in the course. As promised, the timestamps for the sections are:\n\nPart II - Key Takeaways Since Last Time (4:08)\nPart III - Example Problem. (7:36)\nPart IV - Thinking Statistically. (27:26)\nPart V - Outside of STAT 231. (33:25)\nPart VI - Looking Forward. (37:50)\n\nWeek 003: Hi everyone! This weeks tutorial breaks the format that I set out last week (oops!) because I did not want to overwhelm. My solutions to the problems (see attached pdf file!) ended up running long, and so I figured I would save you sitting through the recap/look forward/thinking statistically sections of this video [I had a “magic trick” filmed to explain likelihood, which, if time permits later in the course, I can certainly share (see the attached picture if you want a spoiler!)]. I know that the longer tutorial may feel like additional stress, and so I want to make three things with this clear: (1) I have tried to include as many steps in the solution as I could to provide a very thorough background on the problem. I have tried to talk slow, and not skip details, and so hopefully the longer video is explained by this; (2) I wanted to provide more example problems, particularly this week covering MLE, since they are the first “mathematical topic” we are covering. If you feel very comfortable with them, and you take a look at the questions and feel okay with those, feel free to skim the video – you can certainly do well in this course without it. (3) I will make it up to you with a shorter video on a topic that warrants it! I wanted to remind you all that if you have any questions regarding the content in the tutorials, you can always reach me by email (dylan.spicker@uwaterloo.ca) or on Piazza, and if you have any feedback at all, please feel free to reach out. Ultimately, it is my hope to try to use the tutorials to form some of “in-class” relationship that we are missing out on, being online, and if there is anyway I can improve that for you, please let me know! Thanks, and I hope you all have an excellent weekend and week!\nWeek 004: Hi everyone! Back to a more regular tutorial (and this time much shorter). As promised, the relevant time stamps are included below, and the necessary resources are posted on the course website!\n\n0:00​ - Part I - Introduction and Video Schedule\n1:35​ - Part II - Key Takeaways Since Last Time\n4:50​ - Part III - Example Problem\n20:10​ - Part IV - Build your Intuition\n28:02​ - Part V - Looking Forward\nThe study PDF is uploaded on LEARN and is available: https://www.nejm.org/doi/pdf/10.1056/NEJMoa2035389\nThe protocol PDF is uploaded on LEARN and is available: https://www.nejm.org/doi/suppl/10.1056/NEJMoa2035389/suppl_file/nejmoa2035389_protocol.pdf\nThe qqplot Shiny application is available: https://shiny.math.uwaterloo.ca/sas/stat231/qqplots/\n\nWeek 005: Hi everyone! In this week’s tutorial we are looking at the “Sampling Distribution” (using a giant box of expired chips that my mom gave to me, for some reason), and we investigate some tricks for manipulating likelihood functions (by request from you!). This is a fairly short video before reading week (please, try to enjoy your week away!), and the goal is to prepare you well for our deep dive into intervals in the week back. As always, timestamps are given below. \n\n0:00 - Part 0 - Introduction and Video Schedule\n1:08 - Part I - Example Problem\n9:16 - Part II - Tips and Tricks\n19:47 - Part III - Looking Forward\n\nWeek 006: Hi everyone! This time, our tutorial covers confidence intervals and likelihood intervals. My apologies for the technical issues in the middle! Please reach out if you need more detail in answering the questions! Timestamps!\n\n0:00​ - Part 0 - Introduction.\n1:16​ - Part I - Takeaways Since Last Time.\n2:59​ - Part II - Thinking Statistically.\n12:40​ - Part III - Tips and Tricks.\n16:17​ - Part IV - Example Problem (Sorry!).\n17:20​ - Part V - Looking Forward.\n\nWeek 007\nWeek 008\nWeek 009\nWeek 010\nWeek 011\nWeek 012\nPractice Quiz Solution"
  },
  {
    "objectID": "courses/STAT437/course_index.html",
    "href": "courses/STAT437/course_index.html",
    "title": "STAT 437",
    "section": "",
    "text": "Course Outline - PDF Version\nSTAT 437 - Lectures Overview"
  },
  {
    "objectID": "courses/STAT437/course_index.html#assignments-and-solutions",
    "href": "courses/STAT437/course_index.html#assignments-and-solutions",
    "title": "STAT 437",
    "section": "Assignments and Solutions",
    "text": "Assignments and Solutions\n\nAssignment 1 (Solution)\nAssignment 2 (Solution)\nAssignment 3 (Solution)\nMidterm Test (Solution)\nPaper Review Assignment\nFinal Project"
  },
  {
    "objectID": "courses/STAT437/course_index.html#helper-code-and-supplementary-notes",
    "href": "courses/STAT437/course_index.html#helper-code-and-supplementary-notes",
    "title": "STAT 437",
    "section": "Helper Code and Supplementary Notes",
    "text": "Helper Code and Supplementary Notes\n\nQuasi-Likelihood Theory in Full (Supplementary Notes)\ndata_import_helper.R\nhelper_functions.R"
  },
  {
    "objectID": "courses/STAT437/course_index.html#lecture-videos-slides-notes-and-code",
    "href": "courses/STAT437/course_index.html#lecture-videos-slides-notes-and-code",
    "title": "STAT 437",
    "section": "Lecture Videos, Slides, Notes, and Code",
    "text": "Lecture Videos, Slides, Notes, and Code\n\n(Lecture 001) Welcome to STAT 437\n(Lecture 002) What are longitudinal data?\n\nLecture Slides\n\n(Lecture 003) Exploring Longitudinal Data (Application)\n\nLecture Code\n\n(Lecture 004) Notation for Longitudinal Data (Theory)\n\nLecture Notes\n\n(Lecture 005) What is Linear Regression (Review; Theory)\n\nLecture Slides\n\n(Lecture 006) Continuous Longitudinal Data: Why Can’t we Just Use Regression? (Linear Marginal Models)\n\nLecture Slides\n\n(Lecture 007) Linear Marginal Models: Likelihood, Inference, and Asymptotics (Theory)\n\nLecture Notes\n\n(Lecture 008) Linear Marginal Models: Implementation in R (Application)\n\nLecture Code\n\n(Lecture 009) What are generalized linear models? (Review; Theory)\n\nLecture Slides\n\n(Lecture 010) Marginal Models: Accommodating non-continuous outcomes\n\nLecture Slides\n\n(Lecture 011) M-Estimation: A Practicing Statistician’s Best Friend (Conceptual, Theory, and Application)\n\nLecture Slides\nLecture Notes\n\n(Lecture 012) Generalized Estimating Equations: Estimating parameters from Marginal Models\n\nLecture Slides\n\n(Lecture 013) Generalized Estimating Equations: Examples of GEEs (Theory)\n\nLecture Notes\n\n(Lecture 014) Generalized Estimating Equations: Details of Asymptotic Inference (Theory)\n(Lecture 015) Generalized Estimating Equations: COVID-19 Example\n\nLecture Code\n\n(Lecture 016) Generalized Estimating Equations: Epilepsy Trial Example\n\nLecture Code\n\n(Lecture 017) From the Population to the Individual: Mixed Effects Models\n\nLecture Slides\n\n(Lecture 018) Linear Mixed Effects Models\n\nLecture Slides\n\n(Lecture 019) Linear Mixed Effects Models (Theory)\n\nLecture Notes\n\n(Lecture 020) Variance Testing Considerations: Constrained LRT\n(Lecture 021) Linear Mixed Effects Models (Application)\n\nLecture Code\n\n(Lecture 022) Transition Models for Longitudinal Data\n\nLecture Slides\n\n(Lecture 023) Transition Models (Theory)\n\nLecture Notes\n\n(Lecture 024) Transition Models (Application)\n\nLecture Code\n\n(Lecture 025) Handling Missing Data in Longitudinal Models\n\nLecture Notes\nLecture Slides\n\n(Lecture 026) Handling Missing Data in Longitudinal Models - MCAR, NMAR, and Likelihood Techniques\n\nLecture Code\n\n(Lecture 027) Handling Missing Data in Longitudinal Models - Imputation and Weighting\n\nLecture Code\n\n(Lecture 028) Recap of Longitudinal Methods\n\nLecture Slides\n\n(Lecture 029) Introduction to Time-to-Event Data\n\nLecture Slides\n\n(Lecture 030) Quantities of Interest for Survival Analysis\n\nLecture Notes\n\n(Lecture 031) Discrete Time to Event Data\n\nLecture Slides\n\n(Lecture 032) Discrete Time to Event Data (Theory)\n\nLecture Notes\n\n(Lecture 033) Discrete Time to Event Data Exploration (Application)\n\nLecture Code\n\n(Lecture 034) Logistic Regression and Proportional Odds Models\n\nLecture Slides\n\n(Lecture 035) Logistic Regression and Proportional Odds Models (Application)\n\nLecture Code\n\n(Lecture 036) Introduction to Continuous Time Survival Analysis\n\nLecture Slides\n\n(Lecture 037) Continuous Time Survival Analysis, Likelihood Construction (Theory)\n\nLecture Notes\n\n(Lecture 038) Location Scale Family Distributions, with log-linear Regression (Theory)\n(Lecture 039) Continuous Time Regression Models using Survreg (Application)\n\nLecture Code\n\n(Lecture 040) Accelerated Failure Time Models\n\nLecture Slides\n\n(Lecture 041) Accelerated Failure Time Models (Theory)\n\nLecture Notes\n\n(Lecture 042) Accelerated Failure Time Models (Application)\n\nLecture Code\n\n(Lecture 043) Proportional Hazards Models\n\nLecture Slides\n\n(Lecture 044) Proportional Hazards Models (Theory)\n(Lecture 045) Proportional Hazards Models (Application)\n\nLecture Code"
  },
  {
    "objectID": "courses/STAT437/course_index.html#data-files",
    "href": "courses/STAT437/course_index.html#data-files",
    "title": "STAT 437",
    "section": "Data Files",
    "text": "Data Files\nTo save these data files, right-click and save as. The data are presented for educational purposes only. Sources for the data are available in the data_import_helper.R script, and data should be used only to follow along with the relevant lectures. Any use of this data beyond these purposes needs to be cleared with the data owners."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching History and Course Notes",
    "section": "",
    "text": "Statistical Methods for Life History Analysis\n\n\n\n\n\n\nUniversity of Waterloo\n\n\nWinter 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\nUniversity of Waterloo\n\n\nWinter 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "",
    "text": "Back to 'My Thoughts'\nThis term I had the pleasure of teaching an introductory statistics course at the University of Waterloo. In preparing to run the course I came to realize (and this is by no means a unique realization) that there is a lot of incorrect (or misleading) information aimed at people learning statistics. One particularly stand-out example is the idea that we can use the mean and median of a sample to infer skewness.\nNow, all told this is not the worst offense of this kind, but I think it exemplifies a concerning trend. There is an understandable desire for easy to follow rules in statistics. It is my opinion that, generally speaking, such rules do not exist. This same tendency leads to an uncritical acceptance of arbitrary significance levels and inappropriate applications of approximation theorems. So, in the interest of the bigger picture, let’s correct this misconception!\nThis point has previously been made (von Hippel 2005), but we can compile some explicit examples here."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Positive Skew; Mean < Median",
    "text": "Positive Skew; Mean < Median\nConsider responses to the question ``How many adults (18+) live in this house?’’ for residents across Canada. We can imagine asking 1000 households for this response and it seems plausible that we would have data which look something like:\n\n\n\n\n\nBased on these data, we can see fairly clearly that the median is going to be 2 and that the data exhibit a positive skewness. The mean, however, is given by 1.79 which evidently violates the supposed rule of thumb."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-median-mean",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-median-mean",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Negative Skew; Median < Mean",
    "text": "Negative Skew; Median < Mean\nConsider a quiz that is given to students in an introductory statistics class that is marked out of 5. If we assume that 500 students are enrolled in the course, and they all write the quiz, then the following grade distribution seems plausible:\n\n\n\n\n\nBased on these data, we can see fairly clearly that the median is going to be 4 and that the data exhibit a negative skewness. The mean, however, is given by 4.1 which evidently violates the supposed rule of thumb."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median-1",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median-1",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Positive Skew; Mean = Median",
    "text": "Positive Skew; Mean = Median\n\n\n\nDropping the pretense of real data we can, as may be obvious now, begin to generate arbitrary examples that violate the remaining configuartions of the rules. The following data are selected so that the skew is positive (0.6491939), while the mean (0) and the median (0) are exactly equal.\n\n\n\n\n\nPerhaps the interested reader could come up with an example situation for these data."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-mean-median",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-mean-median",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Negative Skew; Mean = Median",
    "text": "Negative Skew; Mean = Median\n\n\n\nThe previous example can of course be mirrored exactly so as to produce a negative sample skewness (-0.6491939), while the mean (1) and the median (1) are exactly equal.\n\n\n\n\n\nPerhaps the interested reader could come up with an (alternative) example situation for these data."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mean-median-mode",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mean-median-mode",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "No Skew; Mean < Median (< Mode)",
    "text": "No Skew; Mean < Median (< Mode)\n\n\n\nThe remaining scenarios to deconstruct involve distributions which have no skew, but which have a mean and median which are not equal. These are fairly straightforward to construct as well. In this example we can see that the skewness (0) is 0, while the mean (4.5) and the median (5) differ."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mode-median-mean",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mode-median-mean",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "No Skew; (Mode <) Median < Mean",
    "text": "No Skew; (Mode <) Median < Mean\n\n\n\nOnce again, other alternatives are possible, by why reinvent the wheel when we can simply mirror our previous situation? In this example we can see that the skewness (0) is 0, while the mean (5.5) and the median (5) still differ, but differently than last time."
  },
  {
    "objectID": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html",
    "href": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html",
    "title": "Notes on von Mises Calculus for Statistical Functionals: Part I",
    "section": "",
    "text": "Back to 'My Thoughts'\nDisclaimer: These notes are my (attempted, possibly error-prone) summary of Fernholz (1983)."
  },
  {
    "objectID": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#why-might-we-care",
    "href": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#why-might-we-care",
    "title": "Notes on von Mises Calculus for Statistical Functionals: Part I",
    "section": "Why might we care?",
    "text": "Why might we care?\nThe primary reason that we have taken the time to derive the von Mises derivative, influence curve, and so forth is that we can use it for a Taylor-esque expansion of functionals. We write the von Mises expansion of \\(T\\) as \\[T(G) = T(F) + T_F'(G-F) + \\text{Rem}(G-F),\\] where \\(\\text{Rem}(G-F)\\) captures a remainder term.\nIf we take \\(G = F_n\\), then we get \\[\\begin{align*}\n  T(F_n) &= T(F) + T_F'(F_n - F) + \\text{Rem}(F_n - F) \\\\\n  &= T(F) + \\int\\phi_{F}(x)d(F_n - F)(x) + \\text{Rem}(F_n - F) \\\\\n  &= T(F) + \\int\\phi_F(x)dF_n(x) - 0 + \\text{Rem}(F_n - F) \\\\\n  &= T(F) + \\frac{1}{n}\\sum_{i = 1}^n \\phi_F(X_i) + \\text{Rem}(F_n - F).\n\\end{align*}\\]\nNow, asymptotic inference tends to center on the quantity \\(\\widehat{\\theta} - \\theta\\), or in our functional notation, \\(T(F_n) - T(F)\\). Assuming that we have a well behaved influence curve we could apply central limit theorems to \\(\\sqrt{n}(T(F_n) - T(F))\\) to get nice convergence results, so long as \\(\\sqrt{n}\\text{Rem}(F_n - F)\\) converges in probability to \\(0\\). The idea is then to use these expansions to investigate the asymptotic properties.\nThe von Mises derivative, unfortunately, is not sufficient for continuation of this study. It is too weak of a concept to be able to generally conclude that, when it exists, \\(\\sqrt{n}\\text{Rem}(F_n - F) \\stackrel{p}{\\to} 0\\). To overcome this, it is possible to assume that the functional is twice differentiable (with respect to the von Mises derivative), which would enable such conclusions. This is an overly restrictive assumption, and so instead we turn to alternative formulations of the functional derivative.\n\nThis example demonstrates the shortcoming of the von Mises derivative. Consider the functional \\(T(F)\\) that measures the size of the discontinuity points of \\(F\\) on the interval \\([0,1]\\). Take \\(\\alpha > 1\\), then we can write \\[T(F) = \\sum_{x\\in[0,1]} \\left(F(x) - F(x^{-})\\right)^{\\alpha},\\] where \\(F(x^{-}) = \\lim\\limits_{x^*\\to x} F(x^*)\\). On the interval \\([0,1]\\) any distribution function has at most a countable number of discontinuities, so this sum only takes values at a countable number of values, and is as such well-defined (if it helps, define \\(\\mathcal{D}(F)\\) to be the set of discontinuities of \\(F\\) on \\([0,1]\\), and then define the summation for \\(x \\in \\mathcal{D}(F)\\), or similar).\nNow, if we continue \\(F = U\\) to be the distribution function of a \\(\\text{Uniform}(0,1)\\) distribution, then it is clear that \\(T(U) = 0\\). Moreover, the empirical distribution function, computed based on a \\(\\text{Uniform}(0,1)\\) distribution, will almost surely have \\(n\\) discontinuities, each of size \\(\\frac{1}{n}\\). As a result, we will find that \\(T(F_n) = n\\times\\frac{1}{n^\\alpha} = n^{1-\\alpha}\\).\nResultingly, we have that \\(\\sqrt{n}(T(F_n) - T(U)) \\stackrel{a.s}{=} n^{3/2 - \\alpha}\\). However, we can also see that the von Mises derivative exists (and is exactly \\(0\\), so long as \\(\\alpha > 1\\)), which means that for any \\(\\alpha \\in (1, 3/2)\\) we must not have \\(\\text{Rem}(F_n - F) = o_p(n^{-1/2})\\)."
  },
  {
    "objectID": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#a-topological-digression",
    "href": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#a-topological-digression",
    "title": "Notes on von Mises Calculus for Statistical Functionals: Part I",
    "section": "A Topological Digression",
    "text": "A Topological Digression\n\nA topological vector space, given by \\((V(\\mathbb{K}), \\mathcal{T})\\), is a vector space \\(V(\\mathbb{K})\\) on a field \\(\\mathbb{K}\\) (either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)), along with a topology \\(\\mathcal{T}\\), such that addition and scalar multiplication is jointly continuous in both variables. Note, the somewhat unusual notation of \\(V(\\mathbb{K})\\) is just used to stress the field that the vector space is defined on: this will be dropped shortly since I really only care about \\(\\mathbb{K} = \\mathbb{R}\\).\n\nWe introduce the concept of a topological vector space so to build to the concept of the weak and weak\\(*\\) topologies. The idea is to extend the idea of a normed vector space (e.g. \\((V, ||\\cdot||_V)\\)) which can evidently generate its own topology (the norm topology, by taking open balls based on \\(||\\cdot||_V\\)) to allow for weaker (smaller in the set containment sense) topologies. The rationale is that convergence concepts are tied directly (for many convergence notions) to topologies. I will leave this here for now and revist topological notions of convergence (which seems to involve nets – gross).\n\nThe dual space, \\(V^*\\) of a toplogical vector space, \\((V(\\mathbb{K}), \\mathcal{T})\\) is the space of continuous linear functionals from \\(V(\\mathbb{K})\\to\\mathbb{K}\\).\n\nThe dual is continuously brought up in the considerations of functional analysis. There are (apparently) good reasons as to why this is the case. From my perspective, this definition is here mainly as a reminder: for a space \\(V\\) the dual is the space of continuous linear functions from \\(V\\) to the field on which \\(V\\) is defined (the more I say/type it, hopefully the sooner it sticks!).\n\nWe take \\(V\\) to be a topological vector space and \\(V^*\\) to be its dual. Then, the weak topology on \\(V\\) is the topology generated by the seminorms, \\(|\\lambda(x)|\\) for all \\(\\lambda \\in V^*\\). The weak\\(*\\) topology is the topology generated by the seminorms \\(|\\lambda(x)|\\) for all \\(x \\in V\\)."
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html",
    "href": "thoughts/controlling-for-factors/index.html",
    "title": "When Should We Control For Factors?",
    "section": "",
    "text": "Back to 'My Thoughts'"
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html#simpsons-paradox",
    "href": "thoughts/controlling-for-factors/index.html#simpsons-paradox",
    "title": "When Should We Control For Factors?",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nThere is a well known statistical “paradox” Simpson’s Paradox, which discusses the ability for a trend observed in your data to reverse when groups of interest are aggregated. The basic idea is often illustrated with the UC Berkeley Gender Bias example. If you took a look at the admissions data for 1973, you would note that 44% of men who applied, and only 35% of women, were accepted into their respective programs. However, four of the six departments at the school, women were accepted at higher rates than men. The issue is that, on the whole, women were applying to more competitive departments (such as English), where mentended to apply to less competitive disciplines (Engineering and Chemistry). This creates a scenario where there is no explicit discrimination at any department level, but it appears as though there is when aggregated.\nThe lesson from the above, statistically, tends to be condition on relevant quantities. That is, when we are conducting statistical analysis, we ought to “control for” the effects of possibly influential variables; if the above analysis had initially looked at acceptance rates controlling for department of application, a more correct conclusion would have been reached. Seems easy enough - so what is the issue?"
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html#berksons-paradox",
    "href": "thoughts/controlling-for-factors/index.html#berksons-paradox",
    "title": "When Should We Control For Factors?",
    "section": "Berkson’s Paradox",
    "text": "Berkson’s Paradox\n(Prepare for Deja Vu) There is a well known statistical “paradox” Berkson’s Paradox, which discusses the ability for a correlation observed in your data to reverse based on the set of individuals observed. The basic idea is often illustrated with Why Are Handsome Men Such Jerks? example. If you imagine scoring the handsomeness, and the niceness, of men numerically, then perhaps Jim will only date a guy who has some niceness plus handsomeness score that exceeds a threshold. We may say that, in the general population, nicer men also tend to be more handsome (who knows?) so that these two traits are positively correlated. However, among the men that Jim dates, he will observe that nicer men are on average less handsome (and vice-versa). Why? Well, if there is an exceedingly nice man, in order to cross the dating threshold for Jim, this guy does not need to be very handsome at all - and as such, there will be more observations that are extreme than would be expected in the general public.\nThe lesson from the above, statistically, tends to be do not condition on irrelevant quantities. That is, when we are conducting statistical analysis, we ought not “control for” the effects of irrelevant variables; if the above analysis had inititally looked at the pool of men that Jim dates, then we would erroneously conclude that handsomeness and niceness are negatively correlated."
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html#well-now-what",
    "href": "thoughts/controlling-for-factors/index.html#well-now-what",
    "title": "When Should We Control For Factors?",
    "section": "Well, now what?",
    "text": "Well, now what?\nWithout having a subject-matter understanding of the causal structure at play, it is not obvious as to whether a factor is a confounder (e.g. in the first example) or a collider (e.g. in the second example). Confounders should always be conditioned on in a causal analysis, where colliders should never be conditiond on. Doing this incorrectly will lead to incorrect conclusions regarding the presence of a causal effect. This ultimately means that we should not “control for” absolutely every, possible factor; it also ultimately means that we ought to “control for” every relevant factor. Because this cannot be empirically informed, causal inference necessitates importing assumptions from a subject matter expert."
  },
  {
    "objectID": "thoughts/linearity-of-q-functions/index.html",
    "href": "thoughts/linearity-of-q-functions/index.html",
    "title": "On the Linearity of Q-Functions",
    "section": "",
    "text": "A problem of particular focus for the past few days for me has been on attempting to establish when we can make the claim of linearity in the set of Q-functions, based on an outcome model and the relevant distributions of covariates which go into it.\nBriefly, if we define a DTR over \\(K\\) stages, which we wish to fit using Q-learning, we know that the parameter estimates for the Q-functions will be consistent, so long as all of the models are correctly specified. Since each Q-function takes the form \\(Q_k = E[V_{k+1}|H_k]\\), then it is natural to wish to use linear regression models for these functions as they each take the form of a conditional expectation.\nHowever, the quantities \\(V_{k}\\) are given based on the maximization of the previous Q-function (where here, previous actually refers to the \\(k+1\\)-st function), over the treatment parameter, which is binary. As such, these value functions will contain an indicator function times by some linear functional, which of course has the potential to induce severe non-linearity into the Q-functions (particularly as they propagate over time).\nThis problem is more formally discussed in (P. J. Schulte, A. A. Tsiatis, E. B. Laber, and M. Davidian, “Q-and a-learning methods for estimating optimal dynamic treatment regimes,” Statistical Science, vol. 29, no. 4, pp. 640–661,2014.), where they demonstrate that even in a simple scenario, this non-linearity is a problem.\nSo… what? There are two obvious questions that arise from the revelation that Q-functions may, in fact, be non-linear. (1) Are there more flexible models that we can use in place of linear regression? (2) Can we characterize precisely when Q-functions will be linear?\nFor (1) the answer is “yes” and it is done somewhat frequently. This is, of course, a more fruitful avenue of research moving forward, and will certainly be where I turn my attention next. The second question, however, is quite interesting to me.\nIt is certainly possible (in fact, upon some moderate thought, not all that difficult) to define DTRs which do indeed have linear Q-functions; I am interested in seeing whether sufficient (and, perhaps more aspirationally, necessary) conditions are possible."
  },
  {
    "objectID": "thoughts/null-preservation/index.html",
    "href": "thoughts/null-preservation/index.html",
    "title": "Null Preservation",
    "section": "",
    "text": "I am in the process of reading Causal Inference by Hernan and Robins, and one idea has come up a handful of times which strikes me as important - and underdiscussed in a standard statistical curriculum (read: my statistical curriculum).\nThe basic premise is that, under null preservation, the model that we are working with is never misspecified. So instance, if one is considering the null of no causal influece of treatment (\\(a\\)) on an outcome (\\(Y\\)), then the standard quadratic regression \\[E[Y|A=a] = \\beta_0 + \\beta_1a + \\beta_2a^2\\] is correctly specified when there is no causal effect [e.g. it is some constant] even if the mean structure between Y and A is incorrect (supposing it exists).\nThis is a neat concept in part because it means that a conclusion in which we reject the null is valid evidence against the null, regardless of whether or model is correctly specified. If we take a conservative statisticians point-of-view, and never claim evidence in favour of the alternative, then null preservation permits us to draw valid conclusions, even when model mis-specification is all but certain."
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html",
    "href": "thoughts/PSA-causation-without-correlation/index.html",
    "title": "PSA: Causation without Correlation",
    "section": "",
    "text": "Back to 'My Thoughts'"
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#correlation-can-imply-causation",
    "href": "thoughts/PSA-causation-without-correlation/index.html#correlation-can-imply-causation",
    "title": "PSA: Causation without Correlation",
    "section": "Correlation can Imply Causation",
    "text": "Correlation can Imply Causation\nThere are situations where merely observing correlation between two quantities does in fact mean that those two quantities are causally linked. There is an entire field of Statistics dedicated to this distinction. The most common method for detecting causation from correlation is in randomized studies. In a properly conducted, well-powered randomized trial, we can guarantee (with some set degree of confidence) that correlations observed are causal in nature. Of course, as with all findings in Statistics, we can never be certain of the causal nature, but with enough well-collected data, we can be as certain as we need to be.\nEven without randomization, we are able to detect causal relations from observed correlations. These so-called observational studies tend to be more difficult to use for this purpose (e.g. they require stronger assumptions, more data, a sound theoretical backing, or all of the above), they are still frequently and reliably used for this purpose. Consider the case of smoking and lung cancer. It would be entirely unethical to randomly assign groups of individuals (at birth) to be either smokers or non-smokers, and then follow them and collect information regarding the rates of lung-cancer. Instead, we rely on observational data to detect this causal relationship. We have seen enough times (in enough settings) to know that this is a causal relationship. There is a strong scientific backing explaining this phenomenon, and it has reliably held-up over time.\nSo while correlation does not imply causation, it can imply causation, given we take into account enough relevant factors. A Statistics 102 student recognizes this fact, and will often then tell you “correlation does not imply causation, but causation does imply correlation”. Unfortunately, unlike the Statistics 101 student from before, this student is actually simply incorrect. Causation does not imply correlation!"
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#causation-does-not-imply-correlation",
    "href": "thoughts/PSA-causation-without-correlation/index.html#causation-does-not-imply-correlation",
    "title": "PSA: Causation without Correlation",
    "section": "Causation does NOT Imply Correlation",
    "text": "Causation does NOT Imply Correlation\nThis is counter-intuitive (but true). The reason here has to do with the definition of correlation. When people say correlation, they almost certainly mean Pearson’s Correlation, as this is the most commonly used measure of correlation. Pearson’s Correlation is a measure of the linear relationship between two quantities. This is very useful in many settings, but it only takes a moment thought to recognize that non-linear relationships exist. In fact, the following plot would exhibit a correlation of 0.\n\n\n\nLucid Mesh: Non-linear Data\n\n\nNow the two quantities pictured have a very clear relationship, it just happens to be non-linear. As such the correlation is 0. Is the relationship between these two quantities causal? Who knows! But the point is that the observation that correlation equals 0 is a point which gives literally no information as to whether or not a causal relationship exists [I mean, technically it does tell you that it is not possible for it to be a strictly, unmediated linear relationship, but that’s so little information as to basically be no information]."
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#a-set-of-concrete-examples",
    "href": "thoughts/PSA-causation-without-correlation/index.html#a-set-of-concrete-examples",
    "title": "PSA: Causation without Correlation",
    "section": "A Set of Concrete Examples",
    "text": "A Set of Concrete Examples\nTo illustrate the above, we are going to use a set of examples with some real math (don’t worry - it’s not too difficult). These will give you an anchoring point to recognize the difficulty in trying to have a succint statement to capture the interplay between correlation and causation.\n\nTake \\(Y\\) to be some outcome - we will simply call it “utility” so that it is hard to argue against my proposed models.\nTake \\(X\\) to be some measurement about a person - we will simply call it “success” because, again, these are my models! We will assume that \\(X\\) comes before \\(Y\\) in time, so that it could be a cause of \\(Y\\).\nTake \\(S\\) to be a variable representing the sex of an individual, which will take the value of \\(1\\) to represent females and -1 otherwise.\n\n\nCorrelation Implying Causation\nIf we propose that \\(Y = 2X\\) then \\(X\\) and \\(Y\\) have a correlation of \\(1\\). Now, we assume that we have measured everything about these individuals, and this is the only relationship that exists. In this setting, \\(X\\) also directly causes \\(Y\\). That is, an individuals utility is exactly equal to twice their success. Since success comes before utility, success can be said to cause utility.\n\n\nCorrelation without Causation\nNow, let’s say that we observe the exact same model \\(Y = 2X\\), where the correlation is still \\(1\\). Now, we also make the following observations:\n\nFirst, males have a utility of \\(-50\\) and everyone else has a utility of \\(50\\)\nSecond, males have a success of \\(-25\\) and everyone else has a success of \\(25\\)\n\nIn this setting, the exact causal mechanism may not be as clear. However, given the above two observations it seems reasonable to propose that \\(X\\) is caused by \\(S\\), through the model \\(X = 25S\\). Similary, it seems reasonable to propose the model that \\(Y = 50S\\). These two models combine to suggest \\(Y = 2X\\). However, in this setting we have a confounding relationship.\nTo clearly state what is happening: Sex causes utility, and sex causes success. There is no causal relationship between utility and sucess, however, because of the confounder sex it appears as though there is one.\n\n\nCausation without Correlation\nFinally, we propose the model \\(Y = 2AX\\). That is, the utility observed is two times the success of a female, and it is negative two times the success of a male - more successful females have higher utility, more successful males have lower utility.\nIf we assume that there is no relationship between sex and outcome, and we assume that there is an equal probability of being male or female, then we will find that the correlation between utility and success is exactly 0. However, success still causes the utility, it just acts differently on males or females. If you tell me an individuals sex and their success, I can tell you their utility."
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#conclusions",
    "href": "thoughts/PSA-causation-without-correlation/index.html#conclusions",
    "title": "PSA: Causation without Correlation",
    "section": "Conclusions",
    "text": "Conclusions\nSo, what can we say? Well, correlation does not imply causation, except sometimes it does. Further, causation does not imply correlation, unless the relationship is strictly linear. It is certainly less catchy, but also far more accurate, than the standard, so I will understand if it doesn’t catch on!"
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html",
    "href": "thoughts/research-philosophy-statement/index.html",
    "title": "My Research Philosophy Statement (WIP)",
    "section": "",
    "text": "Back to 'My Thoughts'\nNote: This is a work-in-progress, but I figured: “Why not work in the open?”. If you have any thoughts, or feedback, I would of course appreciate that!\nThroughout my PhD, I have been primarily concerned with methodological developments related to measurement error, as it pertains to causal inference generally, and dynamic treatment regimes more specifically. While my work has been predominantly focused on expanding methodology, I have a keen interest in providing a theoretical basis for (comparatively) straightforward methods, which are easy to use for non-statisticians, while exhibiting provably good theoretical properties."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#dissertation-work",
    "href": "thoughts/research-philosophy-statement/index.html#dissertation-work",
    "title": "My Research Philosophy Statement (WIP)",
    "section": "Dissertation Work",
    "text": "Dissertation Work\nBroadly, my research has focused on the fields of causal inference (in particular, precision medicine) and measurement error, where we are conducting the (to our knowledge) first investigation of how the latter impacts the former. Briefly, precision medicine seeks to use  an individuals observed covariates to determine the optimal treatment strategies. These are referred to as adaptive treatment strategies or dynamic treatment regimes (DTRs). That is, precision medicine answers the question “what is the optimal treatment for a patient, when treatment can be”tailored” based on patient-specific information?“. Measurement error refers to any situation where variables of interest are not accurately observed. \nMy dissertation work focused primarily on two, related questions: \n\nHow does the presence of measurement error impact the estimation of optimal adaptive treatment strategies, and can these impacts be remedied?\nHow can existing, commonly used measurement error correction techniques be expanded, to have broader utility for applied researchers?\n\nThe core question of my dissertation came out of the work I completed during my Masters (at the University of Waterloo, under the supervision of Michael Wallace) where we showed that measurement error in tailoring covariates has a substantial impact on the efficacy of estimation of optimal treatment regimes, and correspondingly called into question the use of DTRs estimated with error-prone data for the purpose of future treatment prescription. My PhD work (at the University of Waterloo, supervised by Michael Wallace and Grace Yi) continued on this thread, working on methods to remedy the effects of error in DTRs. We showed that a comparatively simple procedure, regression calibration, can be used to largely overcome the issues introduced with simple covariate measurement error, under fairly mild assumptions (Spicker and Wallace, 2020). \nDoing this required an expansion of the regression calibration technique, in order for it to be applicable to the data on hand. This prompted the secondary question of concern for my dissertation: how can we adapat techniques which are in common use, maintaining their appeal while broadening the scenarios in which they are theoretically defensible? Our first investigation of this line of questioning took as inspiration the generalizations needed for regression calibration in DTRs. In particular, we looked at a class of measurement error correction techniques which rely on “replicate measurements” (that is, independent and identically distributed remeasurements of the quantity of interest) to correct for the effects of measurement error (regression calibration is one such technique). \nWe demonstrated how, with a slight modification to the estimation procedure, these techniques could be applied with any set of independent measurements, whether or not they are identically distributed (Spicker, Wallace, and Yi, 2021). Relaxing this assumption serves as an important contribution as observed data often dramatically contradict the assumption of identically distributed replicates, and these techniques are among the most applied error correction procedures. \nWe continued to expand extant methodologies, through an extension to simulation extrapolation, another frequently used procedure in measurement error correction. Simulation extrapolation relies on a bootstrap-esque, “remeasurement” procedure, and assumes normality of errors for its theoretical results. We demonstrated how this technique can be made nonparametric, removing the need for any assumptions on the error distribution, by “remeasuring” from an empirical distribution instead (Spicker, Wallace, and Yi, 2021). This contribution is particularly valuable, since the assumption of normality is pervasive in the measurement error literature, but there is strong evidence that it is frequently violated in observed data.\nThe final project specifically associated with my dissertation involves questions regarding adherence to treatment regimes. While my work has predominantly looked at errors in continuous variates, it is also an important question to consider what happens when the assigned treatment (typically a binary indicator) may not be accurately observed: that is, some patients do not adhere to the treatment they were assigned. This misclassification problem is of particular importance in the causal inference literature as ignoring it leads to parameter estimates with no substantive, causal interpretation. Presently, my ongoing work is considering ways to recover valid, causal estimators, by modifying DTR estimation techniques rooted in the estimating equation literature."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#non-dissertation-research-work",
    "href": "thoughts/research-philosophy-statement/index.html#non-dissertation-research-work",
    "title": "My Research Philosophy Statement (WIP)",
    "section": "Non-Dissertation Research Work",
    "text": "Non-Dissertation Research Work\nOutside of the work completed for my dissertation, I have also been involved in several intradisciplinary research teams. At the University of Waterloo, I am involved in a group with members from the School of Public Health, and Systems Design Engineering investigating the use of novel machine learning techniques applied to the study of dietary patterns and their implications for health. Dietary data is notoriously error-prone, and machine learning techniques are typically opaque in terms of interpretability. As a result, the simple application of these techniques to these data is unjustified, without a thorough investigation of the ways in which error can be handled, and without substantial improvements to the interpretability and explainability of the machine learning. While the core aim of the group is to work on the applied questions related to health outcomes and health equity, this investigation has branched into several interesting methodological questions which are of interest to pursue. \nPrior to my masters degree, I was involved in another intradiscplinary research team at The Smith School of Business (Queen’s University). The research team involved researchers from Queen’s, in addition to members from the United States, Poland, and Italy. In this team, I provided guidance on the application of machine learning principles to questions of management science. In particular, the group was interested in determining how executives made decisions, whether based predominantly on intuition or analsis. This work was published in a book and has since been expanded into articles (Liebowitz, et al., 2018). While the work itself predates my involvement with statsistics research entirely, the formative experience of intradisciplinary research, with a diverse team, has instilled in me the importance of working diversely to approach interesting and important questions. Moreover, seeing the inner workings of an applied team, demonstrated the utility in ensuring that methodological developments are both theoretically sound, as well as accessible to practitioners, a lesson that I have continued throughout my research to date."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#present-and-future-considerations",
    "href": "thoughts/research-philosophy-statement/index.html#present-and-future-considerations",
    "title": "My Research Philosophy Statement (WIP)",
    "section": "Present and Future Considerations",
    "text": "Present and Future Considerations\nIn the future, I hope to expand the specific areas of research consideration, while maintaining the core underlying principles: developing methodology which is useful for practitioners, theoretically grounded, and tied to interesting questions. Moreover, I hope to continue to expand my multidisciplinary involvement, both as a means of motivating methodological questions and also to ensure that sound statistical theory is applied in the literature.\nWhile there are several avenues to continue investigating measurement error in the context of DTRs (including a more thorough investigation of optimal estimation in the presence of error, for instance), precision medicine presents many interesting and understudied areas of investigation which are of direct interest to me. Some possible lines of inquiry include:\n\nConsidering other forms of noisy data (for instance, missing or censored data), and their impact on optimal DTR estimation;\nConsidering relaxations to assumptions that lay at the heart of causal inference (for instance, how can we accommodate unmeasured confounding);\nConsidering utility-based outcomes, that take into account patient-preferences and alternative outcomes, in place optimizing a single, numerical variable;\nConsidering problems related to the so-called “non-traditional inference”, that arises due to non-differentiability in the estimation procedures;\nConsidering the application of these methods to problems outside of the domain of medicine (for instance, for the purposes of personalized education or in the domain of marketing).\n\nThese problems range from very applied to very theoretical, and could serve as the basis for a research program focused around precision medicine. I fully expect that as the adaptive treatment literature continues to move forward, additional theoretical challenges will arise, providing further ground for investigation. \nOutside of a focus on precision medicine, I also have a particular interest in the statistical development of machine learning techniques, for the purposes of inference rather than prediction. My intradisciplinary work has suggested that, outside of statistics, there is a strong interest in applying state-of-the-art deeplearning techniques in a wide variety of domains. From my perspective, a key limitation in the ability to do this successfully stems from the lack of interpretability, explainability, and statistical basis for these models. To this end, I am also interested in fairly simple questions regarding the statistical basis for these methods (for instance, variable selection techniques, significance testing, model comparisons and tests, and parameter interpretation), with the goal of providing insight into how the methods truly function. The long-term goal with this type of investigation would be to not only ground these novel computational techniques in statistical theory, but to do so to provide a mechanism for using these methods in areas (such as causal inference) which require deeper insight."
  },
  {
    "objectID": "thoughts/teaching-philosophy-statement/index.html",
    "href": "thoughts/teaching-philosophy-statement/index.html",
    "title": "My Teaching Philosophy Statement (WIP)",
    "section": "",
    "text": "Note: This is a work-in-progress, but I figured: “Why not work in the open?”. If you have any thoughts, or feedback, I would of course appreciate that!\nIt is my belief that learning occurs not through the actions of a teacher, but through the actions of students. My role as an instructor is not to impart facts to students. Rather, my job is to facilitate and guide the students through a process of active learning. Practically, this requires capturing and directing students’ attention, aligning their personal objectives with the corresponding course objectives, and creating an environment which is supportive for all learners. In addition to these pillars, I believe in personalizing education for each student, and that feedback and iteration are essential to effective instruction. Although I have had a fair amount of teaching experience in traditional and non-traditional settings, I recognize that I am still in the nascence of my teaching career. Accordingly, I have tried to ground my philosophy not only in what my teaching experience has taught me, but also in the experiences that I have had watching teachers that I admire, and in a voracious consumption of pedagogical research. While this philosophy has served me well so far, this statement also serves as an aspirational standard, and is subject to change as I continue to learn as an instructor.\nMy initial experience in teaching to a diverse group of students began in a non-traditional setting: YouTube videos teaching programming. While the online medium is certainly distinct from a classroom setting, the experience provided me with several important lessons. First, in the online video format, attention and entertainment is key. You have to attract and keep students, which requires selecting interesting topics, framing them in interesting and applicable ways, and doing so with an effective presentation style. For my online teaching, this meant selecting useful and employable topics with insufficient existing coverage online, and then grounding these lessons in real-world, full-scale projects. The idea was to teach the important concepts by building functional applications, keeping the students interested and engaged, and allowing them to build useful portfolio items for themselves. Many individuals showed me projects that had started as a lesson in a video, which they expanded beyond the presented scope, using what they learned for their own purposes. This project-grounded content only became truly successful when I began to polish my presentation skills. Early commenters noted that, while my explanations were clear, I lacked enthusiasm. I worked on the “soft” skills, which shifted the feedback to commending both my presentation and my clarity.\nWhile in a traditional classroom we do not need to attract attention in the same way, I believe that entertaining content encourages students to partake in active learning. If we tell compelling narratives through the course content, students will enthusiastically engage which fosters learning. In statistics there are plenty of ways to catch and hold the attention of our students. During lessons, I try to use topical, relevant examples. For instance, I used COVID-19 vaccine trials to teach statistical research frameworks in the Winter of 2021, to great success. During office hours I deeply connected with some students who had an interest in sports by showcasing methods on data from the National Hockey League (NHL).\nI have applied the same philosophy to creating assessments. At Queen’s University, I worked with two professors to design and implement a cross-discipline course. In the course, groups of business and computing majors worked to build an app-based business. The students demoed and pitched their completed apps to real-world entrepreneurs. This unified all the course concepts in a fun and applicable project. In an upcoming, fourth-year, statistics course I am giving freedom on the “standard”, problem-set assignments. For the advanced material, students can opt to take a theoretical or applied view, depending on their ultimate goals. These techniques have generated considerable, excited feedback from students, who are eager to engage with the topics.\nAfter attracting a student’s attention with the goal of directing it, the natural question is “at what”? I believe that it is important to have clear learning objectives, both at a macro (course) level, and at a micro (lesson or assignment) level. These objectives should be comprehensibly presented to students, and consistently aligned with all aspects of the course. For instance, when introducing marginal linear models, my lesson begins by looking at the structure of the data that we wish to analyze. This states a clear objective: analyze these data. Then, I ask the students to consider why we need new methods to analyze these data. I ask them to think about why the knowledge that they currently have (linear regression) is insufficient for this task. This leads to them considering the ways that they need to expand their knowledge to achieve the outlined goal. Here, discovery and the synthesis of new concepts with existing knowledge form the center of the lesson. I try to use this approach wherever possible. Based on student feedback, this framing has been effective at getting ideas to stick. However, course objectives are not the only relevant objectives for my teaching.\nFor better or worse, each student enters the classroom with objectives of their own. For most students, learning the course material is a means to an end. Their true interest is in, for instance, obtaining a specific job or getting into graduate school. These goals are not the primary consideration for us as instructors, but we must recognize that they are the primary consideration for our students. Doing so allows us to leverage an important resource: intrinsic motivation. Students are more likely to put in the deliberate effort required for learning, if it is in service of their goals. For instance, many of the students in my cross-discipline course expressed interest in start-up work. The project served as an important portfolio item for those students, providing direct experience in the field. Further, it introduced the students to industry contacts, building their network. This led the students to invest significant time and effort into their projects, and benefit from the learning that took place alongside it. Additionally, the students expressed satisfaction with the course and with our assessment format. I intend to take a similar approach with the final project in my upcoming longitudinal data analysis course. The cumulative project will give students a chance to apply the course concepts, producing work that can range from a data science portfolio piece through to an academic research poster. My hope is that the students will use the work they are doing in the course to advance their aspirations in a way which is more useful than a simple, numeric grade. In doing so, their intrinsic motivation should help carry their efforts, and promote deeper understanding.\nThis strategy of marrying course objectives and student objectives comes with two additional benefits. First, it provides the opportunity for personalization. I have tried to do this through personal surveys, soliciting interests of students, to use in tutorial lessons or lectures. The choice afforded to students in their projects and assignments also serves to tailor the experience. Enhanced personalization remains a focus of mine moving forward. Second, there is ample evidence that extrinsic motivators (such as grades) are not conducive to learning. Providing assessments that have value beyond their contribution to a final mark minimizes the importance of grades. This has benefits in the ability to keep a student interested and motivated. Additionally, it assists in creating a healthy learning environment, the third pillar of my teaching philosophy.\nThe environment created in a course sets the tone for a student’s willingness to engage, which in my view, is a key factor in their ability to learn. To me, the learning environment needs to be welcoming and safe for all individuals, with specific attention paid to mental well-being. The environment should also be relaxed and feel low-stakes, and encourage mistakes. Some of this can be achieved, passively and by example. For instance, I include my pronouns of “they/them” on the course syllabus, and in my signature when emailing students. This is a simple practice which garners grateful messages, particularly from students in marginalized groups. In my YouTube teaching, I keep mistakes (with a discussion of the correction) in the final videos. This teaches the skill of debugging, but also lessens the sting a student feels when making a mistake. Doing this has led to substantially more discussion of the students’ mistakes than was present before I started including mine. Other techniques for creating a positive environment require more deliberate action. For instance, in an introductory statistics course we provided the students with a flexible grading scheme, dropping their worst marks from their final grade. This helped alleviate the stress of any individual assessment. Additionally, we provided a mock exam, alongside a video lesson of me solving the exam, and discussing my thought process throughout. Students indicated that this reduced their exam anxiety. This has positive benefits for learning, despite the fact that the mock exam had little bearing on the true final exam. In my fourth year longitudinal data course, I allow students to re-submit corrected assignments alongside explanations of what their mistakes were, for partial credit. This policy reinforces the idea that mistakes are an integral part of learning, and will not be punished. Participation is an effective technique to have students actively learning. However, “random call” and “forced participation” both create high-stress settings for many students. Anonymous participation methods allow for a similar level of direct engagement, without the high-stress drawbacks. I have had success using “iClickers” and “TopHat” when teaching in person, and “Kahoot!” when teaching online. Students have been almost universally thankful for the accommodations I provide them and for the environment I create. While these principles have served me well in my teaching thus far there is room for growth in my abilities as a teacher. My non-traditional start to teaching provided one additional lesson: feedback is key. Hundreds of thousands of individuals have seen my videos, and thousands have left feedback on them. This feedback has ranged from comments regarding my presentation, to suggestions about ways of clarifying particular topics. It has included positive feedback about the framing devices used, the narratives I create, and the projects I present. This feedback improved my videos over the course of my online teaching, and it has made clear to me how important feedback is to effective teaching. Going forward, I intend to make generous use of feedback from students, both through anonymous surveys during the term and official teaching assessments, as well as from my colleagues.\nI have enjoyed success as an instructor. I have made meaningful connections with my students, received plenty of praise, and have been recognized for this with several opportunities to continue teaching. However, the core component of my teaching philosophy is that I can always improve. And I look forward to continuing to learn, grow, and practice my skills as an instructor."
  }
]