---
title: The (lack of a) Relationship between Mean, Median, and Skewness
author: 'Dylan Spicker'
date: '2021-04-20'
slug: []
categories: [statistics, teaching, misconceptions]
tags: [statistics, teaching, misconceptions]
description: "There is a commonly taught 'rule-of-thumb' that states that we can determine the skewness of a distribution based on the relative location of the mean and the median. This is not true."
image: 'Relationship_between_mean_and_median_under_different_skewness.png'
references:
- id: article1
  title: "Mean, Median, and Skew: Correcting a Textbook Rule"
  author:
  - family: "von Hippel"
    given: "Paul T"
  container-title: "Journal of Statistics Education"
  volume: 13
  URL: 'http://dx.doi.org/10.1080/10691898.2005.11910556'
  DOI: 10.1080/10691898.2005.11910556
  issue: 2
  publisher: 'Taylor & Francis'
  page: null
  type: article-journal
  issued:
    year: 2005
- id: article2
  title: "The Mean, Median, Mode Inequality and Skewness for a Class of Densities"
  author:
  - family: "MacGillivray"
    given: "H. L."
  container-title: "Australian Journal of Statistics"
  volume: 23
  URL: 'https://doi.org/10.1111/j.1467-842X.1981.tb00784.x'
  DOI: 10.1080/10691898.2005.11910556
  issue: 2
  publisher: 'Statistical Society of Australia'
  page: 247-250
  type: article-journal
  issued:
    year: 1981
---

This term I had the pleasure of teaching an introductory statistics course at the University of Waterloo. In preparing to run the course I came to realize (and this is by no means a unique realization) that there is **a lot** of incorrect (or misleading) information aimed at people learning statistics. One particularly stand-out example is the idea that we can use the mean and median of a sample to infer skewness.

> If the mean is less than the median the data exhibit a left (negative) skew. If the mean is greater than the median the data exhibit a right (positive) skew. `r tufte::quote_footer('--- Introductory Textbooks, sometimes')`

Now, all told this is not the worst offense of this kind, but I think it exemplifies a concerning trend. There is an understandable desire for easy to follow rules in statistics. It is my opinion that, generally speaking, such rules do not exist. This same tendency leads to an uncritical acceptance of arbitrary significance levels and inappropriate applications of approximation theorems. So, in the interest of the bigger picture, let's correct this misconception!

This point has previously been made [@article1], but we can compile some explicit examples here.

## Positive Skew; Mean < Median
Consider responses to the question ``How many adults (18+) live in this house?'' for residents across Canada. We can imagine asking 1000 households for this response and it seems plausible that we would have data which look something like:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
theme_update(plot.background = element_rect(fill = "#f8f9fa",colour = NA))
n <- 1000
p1 <- 0.38
p2 <- 0.49
p3 <- 0.1
p4 <- 0.02
p5 <- 0.01
household_responses <- c(rep(1, n*p1), rep(2, n*p2), rep(3, n*p3), rep(4, n*p4), rep(5, n*p5))
data.frame(y1 = household_responses) %>% 
  mutate("Number of Adults"=y1) %>% 
  ggplot(aes(x=`Number of Adults`)) + 
  geom_histogram(binwidth=1) + 
  labs(title = "How many adults (18+) live in this house?", y="Number of Respondents")
```

Based on these data, we can see fairly clearly that the median is going to be `r median(household_responses)` and that the data exhibit a positive skewness. The mean, however, is given by `r mean(household_responses)` which evidently violates the supposed rule of thumb.

## Negative Skew; Median < Mean
Consider a quiz that is given to students in an introductory statistics class that is marked out of 5. If we assume that 500 students are enrolled in the course, and they all write the quiz, then the following grade distribution seems plausible:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
n <- 500
p5 <- 0.37
p4 <- 0.45
p3 <- 0.11
p2 <- 0.05
p1 <- 0.02
grade_responses <- c(rep(1, n*p1), rep(2, n*p2), rep(3, n*p3), rep(4, n*p4), rep(5, n*p5))
data.frame(y1 = grade_responses) %>% 
  mutate("Quiz Grade"=y1) %>% 
  ggplot(aes(x=`Quiz Grade`)) + 
  geom_histogram(binwidth=1) + 
  labs(title = "Quiz Grades (out of 5).", y="Number of Students")
```

Based on these data, we can see fairly clearly that the median is going to be `r median(grade_responses)` and that the data exhibit a negative skewness. The mean, however, is given by `r mean(grade_responses)` which evidently violates the supposed rule of thumb.


## Positive Skew; Mean = Median
```{r, echo=FALSE, warning=FALSE, message=FALSE}
n <- 1000
p <- 1/6
change_by <- 2
weird_skew <- c(rep(-1, n*(1 - 3*p) - change_by), rep(0, n*p + change_by), rep(1, n*p), rep(2, n*p))
```
Dropping the pretense of real data we can, as may be obvious now, begin to generate arbitrary examples that violate the remaining configuartions of the rules. The following data are selected so that the skew is positive (`r e1071::skewness(weird_skew)`), while the mean (`r mean(weird_skew)`) and the median (`r median(weird_skew)`) are exactly equal. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data.frame(y1 = weird_skew) %>% 
  mutate("Arbitrary Variate"=y1) %>% 
  ggplot(aes(x=`Arbitrary Variate`)) + 
  geom_histogram(binwidth=1) + 
  labs(title = "No particular scenario.", y="Frequencies")
```

Perhaps the interested reader could come up with an example situation for these data. 

## Negative Skew; Mean = Median
```{r, echo=FALSE, warning=FALSE, message=FALSE}
n <- 1000
p <- 1/6
change_by <- 2
weird_skew_two <- c(rep(2, n*(1 - 3*p) - change_by), rep(1, n*p + change_by), rep(0, n*p), rep(-1, n*p))
```
The previous example can of course be mirrored exactly so as to produce a negative sample skewness (`r e1071::skewness(weird_skew_two)`), while the mean (`r mean(weird_skew_two)`) and the median (`r median(weird_skew_two)`) are exactly equal. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data.frame(y1 = weird_skew_two) %>% 
  mutate("Arbitrary Variate"=y1) %>% 
  ggplot(aes(x=`Arbitrary Variate`)) + 
  geom_histogram(binwidth=1) + 
  labs(title = "No particular scenario (number two).", y="Frequencies")
```

Perhaps the interested reader could come up with an (alternative) example situation for these data. 

## No Skew; Mean < Median (< Mode)
```{r, echo=FALSE, warning=FALSE, message=FALSE}
n <- 3700
cat1 <- 1; prop1 <- 281/n
cat2 <- 2; prop2 <- 124/n
cat3 <- 3; prop3 <- 1/n
cat4 <- 4; prop4 <- 84/n
cat5 <- 5; prop5 <- 19/n
cat6 <- 6; prop6 <- 100/n
cat7 <- 7; prop7 <- 290/n
cat8 <- 9; prop8 <- 100/n

symmetric_mean_median <- c(rep(cat1, prop1*n),
                           rep(cat2, prop2*n),
                           rep(cat3, prop3*n),
                           rep(cat4, prop4*n),
                           rep(cat5, prop5*n),
                           rep(cat6, prop6*n),
                           rep(cat7, prop7*n),
                           rep(cat8, prop8*n))
  
```
The remaining scenarios to deconstruct involve distributions which have no skew, but which have a mean and median which are not equal. These are fairly straightforward to construct as well. In this example we can see that the skewness (`r round(e1071::skewness(symmetric_mean_median), 4)`) is 0, while the mean (`r round(mean(symmetric_mean_median), 4)`) and the median (`r median(symmetric_mean_median)`) differ. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data.frame(y1 = symmetric_mean_median) %>% 
  mutate("Arbitrary Variate"=y1) %>% 
  ggplot(aes(x=`Arbitrary Variate`)) + 
  geom_histogram(binwidth=1) + 
  scale_x_discrete(name = "Some Variate", limits=factor(1:9)) +
  labs(title = "No particular scenario (number three).", y="Frequencies")
```

## No Skew; (Mode <) Median < Mean
```{r, echo=FALSE, warning=FALSE, message=FALSE}
n <- 3700
cat1 <- 9; prop1 <- 281/n
cat2 <- 8; prop2 <- 124/n
cat3 <- 7; prop3 <- 1/n
cat4 <- 6; prop4 <- 84/n
cat5 <- 5; prop5 <- 19/n
cat6 <- 4; prop6 <- 100/n
cat7 <- 3; prop7 <- 290/n
cat8 <- 1; prop8 <- 100/n

symmetric_mean_median_two <- c(rep(cat1, prop1*n),
                           rep(cat2, prop2*n),
                           rep(cat3, prop3*n),
                           rep(cat4, prop4*n),
                           rep(cat5, prop5*n),
                           rep(cat6, prop6*n),
                           rep(cat7, prop7*n),
                           rep(cat8, prop8*n))
  
```
Once again, other alternatives are possible, by why reinvent the wheel when we can simply mirror our previous situation? In this example we can see that the skewness (`r round(e1071::skewness(symmetric_mean_median_two), 4)`) is 0, while the mean (`r round(mean(symmetric_mean_median_two), 4)`) and the median (`r median(symmetric_mean_median_two)`) still differ, but differently than last time. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data.frame(y1 = symmetric_mean_median_two) %>% 
  mutate("Arbitrary Variate"=y1) %>% 
  ggplot(aes(x=`Arbitrary Variate`)) + 
  geom_histogram(binwidth=1) + 
  scale_x_discrete(name = "Some Variate", limits=factor(1:9)) +
  labs(title = "No particular scenario (number four).", y="Frequencies")
```

# General Lessons and Takeaways
These examples tended to exploit the fact that discrete distributions behave strangely. We could have also used multimodal distributions to a similar effect. While these distributions often feel contrived when compared to our nice, named distributions, I would wager that histograms like those presented are, in fact, more common in an *actual* data analysis than the nice, smooth, well-behaved densities we like to work with.

The keen reader may wish to pushback and claim that adding in the relationship with the mode will remedy these supposed counter examples -- unfortunately, no. For one, especially when working with continuous densities, it becomes trivial to add a unique mode anywhere relative to the mean and median, without substantively changing these values. Second, while the first few examples had their mode coinciding with the median, the last two demonstrate that we can have a strict ordering (Mean < Median < Mode or Mode < Median < Mean) and end up without any skew in our data.

So, are there any rules we can follow? If the distribution is Pearson family, the standard rule does apply [@article2]. Beyond that, I think that the common refrain is more helpful as an intuition than it is as a rule, and I think that this is true of many statistical absolutes that are commonly taught. 

## **References**















