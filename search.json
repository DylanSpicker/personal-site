[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My Google Scholar has an up to date set of my publications and preprints.\nMy YouTube Channel contains lecture and tutorial videos from courses I have taught, illustrating my teaching style.\nMy CV is available in PDF form (last updated October, 2022).\nMy Teaching Philosophy Statement has been published as a blog post.\nMy Research Philosophy Statement has been published as a blog post."
  },
  {
    "objectID": "about.html#relevant-links",
    "href": "about.html#relevant-links",
    "title": "About Me",
    "section": "",
    "text": "My Google Scholar has an up to date set of my publications and preprints.\nMy YouTube Channel contains lecture and tutorial videos from courses I have taught, illustrating my teaching style.\nMy CV is available in PDF form (last updated October, 2022).\nMy Teaching Philosophy Statement has been published as a blog post.\nMy Research Philosophy Statement has been published as a blog post."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "About Me",
    "text": "About Me\nMy research focuses on areas of causal inference, and specifically methodologies related to dynamic treatment regimes. During my graduate studies, my research focused on measurement error and causal inference. Briefly, measurement error occurs whenever we are interested in measuring something and we do a bad job of it. This happens in almost every study that is run, and unfortunately means that the conclusions that we draw may not be accurate: statistical work on measurement error tries to correct this. Causal inference asks questions of the form “Does X cause Y?” [For instance “Does smoking cause lung cancer?” (yes, it does).] I have a keen interest in providing a theoretical basis for (comparatively) straightforward methods, which are easy to use for non-statisticians, while exhibiting provably good theoretical properties. During my postdoc, I explored problems related to privacy and dynamic treatment regimes, where I sought to determine ways that individual’s personal health data can be protected, while gleaning the useful insights that we seek.\nOutside of causal inference and measurement error, I am interested in machine learning, and in particular in trying to establish a statistical basis for novel machine learning techniques (including questions related to inference, interpretability, and model selection).\nI previously did an undergraduate degree in Finance and Mathematics at Queen’s University (I transferred there after completing my first year at Waterloo/Laurier in the ‘Double Degree’ program), and a Master’s of Statistics at Waterloo.\nOutside of my research, I pay very close attention to sports, mostly hockey, (and how statistics is, or should be, applied there), play music (without any connection to statistics), and enjoy board/video games (with varying degrees of statistical relevance). I have a cat (Charles) who is wonderful."
  },
  {
    "objectID": "about.html#my-teaching",
    "href": "about.html#my-teaching",
    "title": "About Me",
    "section": "My Teaching",
    "text": "My Teaching\n\nIn the Winter of 2022 I taught STAT 437 at the University of Waterloo. The course was offered online, and the material is available on my website.\nIn the Winter of 2021 I was one of the instructors for STAT 231 at the University of Waterloo. As a part of my role for this I produced a set of weekly tutorial videos. These are available to view in a YouTube playlist.\nI have served as a teaching assistant for 18 courses between my time at the University of Waterloo and Queen’s University. A full list is available in my CV."
  },
  {
    "objectID": "about.html#contacting-me",
    "href": "about.html#contacting-me",
    "title": "About Me",
    "section": "Contacting Me",
    "text": "Contacting Me\n\nEmail: See my departmental website.\nLinkedIn: In case anyone is still using this."
  },
  {
    "objectID": "courses/STAT231/course_index.html",
    "href": "courses/STAT231/course_index.html",
    "title": "STAT 231",
    "section": "",
    "text": "Week 002: This tutorial goes over the concepts from weeks one and two in the course. As promised, the timestamps for the sections are:\n\nPart II - Key Takeaways Since Last Time (4:08)\nPart III - Example Problem. (7:36)\nPart IV - Thinking Statistically. (27:26)\nPart V - Outside of STAT 231. (33:25)\nPart VI - Looking Forward. (37:50)\n\nWeek 003: Hi everyone! This weeks tutorial breaks the format that I set out last week (oops!) because I did not want to overwhelm. My solutions to the problems (see attached pdf file!) ended up running long, and so I figured I would save you sitting through the recap/look forward/thinking statistically sections of this video [I had a “magic trick” filmed to explain likelihood, which, if time permits later in the course, I can certainly share (see the attached picture if you want a spoiler!)]. I know that the longer tutorial may feel like additional stress, and so I want to make three things with this clear: (1) I have tried to include as many steps in the solution as I could to provide a very thorough background on the problem. I have tried to talk slow, and not skip details, and so hopefully the longer video is explained by this; (2) I wanted to provide more example problems, particularly this week covering MLE, since they are the first “mathematical topic” we are covering. If you feel very comfortable with them, and you take a look at the questions and feel okay with those, feel free to skim the video – you can certainly do well in this course without it. (3) I will make it up to you with a shorter video on a topic that warrants it! I wanted to remind you all that if you have any questions regarding the content in the tutorials, you can always reach me by email (dylan.spicker@uwaterloo.ca) or on Piazza, and if you have any feedback at all, please feel free to reach out. Ultimately, it is my hope to try to use the tutorials to form some of “in-class” relationship that we are missing out on, being online, and if there is anyway I can improve that for you, please let me know! Thanks, and I hope you all have an excellent weekend and week!\nWeek 004: Hi everyone! Back to a more regular tutorial (and this time much shorter). As promised, the relevant time stamps are included below, and the necessary resources are posted on the course website!\n\n0:00​ - Part I - Introduction and Video Schedule\n1:35​ - Part II - Key Takeaways Since Last Time\n4:50​ - Part III - Example Problem\n20:10​ - Part IV - Build your Intuition\n28:02​ - Part V - Looking Forward\nThe study PDF is uploaded on LEARN and is available: https://www.nejm.org/doi/pdf/10.1056/NEJMoa2035389\nThe protocol PDF is uploaded on LEARN and is available: https://www.nejm.org/doi/suppl/10.1056/NEJMoa2035389/suppl_file/nejmoa2035389_protocol.pdf\nThe qqplot Shiny application is available: https://shiny.math.uwaterloo.ca/sas/stat231/qqplots/\n\nWeek 005: Hi everyone! In this week’s tutorial we are looking at the “Sampling Distribution” (using a giant box of expired chips that my mom gave to me, for some reason), and we investigate some tricks for manipulating likelihood functions (by request from you!). This is a fairly short video before reading week (please, try to enjoy your week away!), and the goal is to prepare you well for our deep dive into intervals in the week back. As always, timestamps are given below. \n\n0:00 - Part 0 - Introduction and Video Schedule\n1:08 - Part I - Example Problem\n9:16 - Part II - Tips and Tricks\n19:47 - Part III - Looking Forward\n\nWeek 006: Hi everyone! This time, our tutorial covers confidence intervals and likelihood intervals. My apologies for the technical issues in the middle! Please reach out if you need more detail in answering the questions! Timestamps!\n\n0:00​ - Part 0 - Introduction.\n1:16​ - Part I - Takeaways Since Last Time.\n2:59​ - Part II - Thinking Statistically.\n12:40​ - Part III - Tips and Tricks.\n16:17​ - Part IV - Example Problem (Sorry!).\n17:20​ - Part V - Looking Forward.\n\nWeek 007\nWeek 008\nWeek 009\nWeek 010\nWeek 011\nWeek 012\nPractice Quiz Solution"
  },
  {
    "objectID": "courses/STAT231/course_index.html#weekly-tutorials",
    "href": "courses/STAT231/course_index.html#weekly-tutorials",
    "title": "STAT 231",
    "section": "",
    "text": "Week 002: This tutorial goes over the concepts from weeks one and two in the course. As promised, the timestamps for the sections are:\n\nPart II - Key Takeaways Since Last Time (4:08)\nPart III - Example Problem. (7:36)\nPart IV - Thinking Statistically. (27:26)\nPart V - Outside of STAT 231. (33:25)\nPart VI - Looking Forward. (37:50)\n\nWeek 003: Hi everyone! This weeks tutorial breaks the format that I set out last week (oops!) because I did not want to overwhelm. My solutions to the problems (see attached pdf file!) ended up running long, and so I figured I would save you sitting through the recap/look forward/thinking statistically sections of this video [I had a “magic trick” filmed to explain likelihood, which, if time permits later in the course, I can certainly share (see the attached picture if you want a spoiler!)]. I know that the longer tutorial may feel like additional stress, and so I want to make three things with this clear: (1) I have tried to include as many steps in the solution as I could to provide a very thorough background on the problem. I have tried to talk slow, and not skip details, and so hopefully the longer video is explained by this; (2) I wanted to provide more example problems, particularly this week covering MLE, since they are the first “mathematical topic” we are covering. If you feel very comfortable with them, and you take a look at the questions and feel okay with those, feel free to skim the video – you can certainly do well in this course without it. (3) I will make it up to you with a shorter video on a topic that warrants it! I wanted to remind you all that if you have any questions regarding the content in the tutorials, you can always reach me by email (dylan.spicker@uwaterloo.ca) or on Piazza, and if you have any feedback at all, please feel free to reach out. Ultimately, it is my hope to try to use the tutorials to form some of “in-class” relationship that we are missing out on, being online, and if there is anyway I can improve that for you, please let me know! Thanks, and I hope you all have an excellent weekend and week!\nWeek 004: Hi everyone! Back to a more regular tutorial (and this time much shorter). As promised, the relevant time stamps are included below, and the necessary resources are posted on the course website!\n\n0:00​ - Part I - Introduction and Video Schedule\n1:35​ - Part II - Key Takeaways Since Last Time\n4:50​ - Part III - Example Problem\n20:10​ - Part IV - Build your Intuition\n28:02​ - Part V - Looking Forward\nThe study PDF is uploaded on LEARN and is available: https://www.nejm.org/doi/pdf/10.1056/NEJMoa2035389\nThe protocol PDF is uploaded on LEARN and is available: https://www.nejm.org/doi/suppl/10.1056/NEJMoa2035389/suppl_file/nejmoa2035389_protocol.pdf\nThe qqplot Shiny application is available: https://shiny.math.uwaterloo.ca/sas/stat231/qqplots/\n\nWeek 005: Hi everyone! In this week’s tutorial we are looking at the “Sampling Distribution” (using a giant box of expired chips that my mom gave to me, for some reason), and we investigate some tricks for manipulating likelihood functions (by request from you!). This is a fairly short video before reading week (please, try to enjoy your week away!), and the goal is to prepare you well for our deep dive into intervals in the week back. As always, timestamps are given below. \n\n0:00 - Part 0 - Introduction and Video Schedule\n1:08 - Part I - Example Problem\n9:16 - Part II - Tips and Tricks\n19:47 - Part III - Looking Forward\n\nWeek 006: Hi everyone! This time, our tutorial covers confidence intervals and likelihood intervals. My apologies for the technical issues in the middle! Please reach out if you need more detail in answering the questions! Timestamps!\n\n0:00​ - Part 0 - Introduction.\n1:16​ - Part I - Takeaways Since Last Time.\n2:59​ - Part II - Thinking Statistically.\n12:40​ - Part III - Tips and Tricks.\n16:17​ - Part IV - Example Problem (Sorry!).\n17:20​ - Part V - Looking Forward.\n\nWeek 007\nWeek 008\nWeek 009\nWeek 010\nWeek 011\nWeek 012\nPractice Quiz Solution"
  },
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Research Work, Presentations, and Grants",
    "section": "",
    "text": "Nonparametric Simulation Extrapolation for Measurement Error Models\n\n\n\n\n\nPublished in the Canadian Journal of Statistics. (Open Access)\n\n\n\n\n\n\nJune 27, 2023\n\n\nDylan Spicker, Michael P. Wallace, and Grace Y. Yi\n\n\n\n\n\n\n  \n\n\n\n\nPreserving Patient Privacy in Dynamic Treatment Regimes.\n\n\n\n\n\nInvited talk given at the Statistical Society of Canada general meeting (2023, Ottawa). (PDF of Slides)\n\n\n\n\n\n\nMay 30, 2023\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nNew Brunswick Innovation Fund - Talent Recruiment Fund\n\n\n\n\n\nNBIF TRF award for $55,000 over a two-year period.\n\n\n\n\n\n\nMarch 27, 2023\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nNonadherence in Dynamic Treatment Regimes – Moving Beyond Intention-to-Treat Analyses.\n\n\n\n\n\nInvited talk given for the McGill University Biostatistics Seminars (2022, Montreal). (PDF of Slides)\n\n\n\n\n\n\nOctober 19, 2022\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nStatLab-CANSSI-CRM Postdoctoral Fellowship\n\n\n\n\n\nPostdoctoral financing funded through CANSSI and StaLab CRM. 1 year of postdoctoral funding.\n\n\n\n\n\n\nSeptember 1, 2022\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nOut-to-Innovate Research Fellowship\n\n\n\n\n\nOut-to-innovate merit based research scholarship; $5000 USD value. Details.\n\n\n\n\n\n\nSeptember 1, 2022\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nNonparametric Simulation Extrapolation for Measurement Error Models.\n\n\n\n\n\nTalk given at the Statistical Society of Canada general meeting (2022, Digital Conference [Thanks, COVID]). (PDF of Slides)\n\n\n\n\n\n\nJune 2, 2022\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nThe Harms of Historical Hyperbole.\n\n\n\n\n\nTalk given at the Statistics and Actuarial Sciences Student Seminar Series (SASSSS). (2021, Waterloo). (PDF of Slides. Note, these slides were originally presented as an interactive presentation, rather than PDF, so some is lost in translation. I am working on figuring out how to display the presentation here better! Apologies.)\n\n\n\n\n\n\nJune 29, 2021\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nMeasurement Error Corrections with Non-IID Auxiliary Data.\n\n\n\n\n\nTalk given at the Statistical Society of Canada general meeting (2021, Digital Conference [Thanks, COVID]). (PDF of Slides)\n\n\n\n\n\n\nJune 10, 2021\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nGeneralizations to Corrections for the Effects of Measurement Error in Approximately Consistent Methodologies (Preprint)\n\n\n\n\n\nSubmitted for consideration (September 2021). (Arxiv Preprint)\n\n\n\n\n\n\nMay 25, 2021\n\n\nDylan Spicker, Michael P. Wallace, and Grace Y. Yi\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Society of Canada SARGC Video Competition 2021.\n\n\n\n\n\nWinning Team; Video competition, explaining the importance of statistics to a lay audience. Our video is available: ‘Using Statistics to break down COVID-19 misconceptions.’\n\n\n\n\n\n\nMarch 2, 2021\n\n\nDylan Spicker and Melissa Van Bussel\n\n\n\n\n\n\n  \n\n\n\n\nMeasurement error and precision medicine: Error‐prone tailoring covariates in dynamic treatment regimes.\n\n\n\n\n\nPublished in Statistics in Medicine (August 2020). https://doi.org/10.1002/sim.8690 (Arxiv Preprint)\n\n\n\n\n\n\nAugust 4, 2020\n\n\nDylan Spicker and Michael Wallace\n\n\n\n\n\n\n  \n\n\n\n\nNSERC Postgraduate Scholarship - Doctoral (PGS-D)\n\n\n\n\n\n$21,000 / year; 3 years. Receive a competitive, 3 year award from NSERC, with added funds matched by department. Fully funded my PhD studies.\n\n\n\n\n\n\nMay 1, 2020\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nOGS Queen Elizabeth II Graduate Scholarship in Science and Technology (QEII-GSST).\n\n\n\n\n\n$15,000/year; 1 year. Received a competitive, 1 year award through OGS.\n\n\n\n\n\n\nMay 1, 2020\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Society of Canada Case Studies in Data Analysis Competition 2020.\n\n\n\n\n\nWinning Team; Case study competition for the SSC’s annual case study competition. Our project was titled: Forecasting iTunes popularity out-of-sample: in-sample validation for longitudinal and matching methods.\n\n\n\n\n\n\nMay 1, 2020\n\n\nDylan Spicker and Melissa Van Bussel\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Treatment Regimes and Measurement Error: A non-technical overview (where we mostly talk about video games!).\n\n\n\n\n\nTalk given at the University of Waterloo Undergraduate Mathematics Research Conference. (PDF of Slides)\n\n\n\n\n\n\nSeptember 28, 2019\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nCANSSI National Case Study Competition 2019.\n\n\n\n\n\nRegional Winning Team; National Finalist; Case study competition for CANSSI’s annual case study competition. Our project was titled: ’SuperLearn’ed Delays: Partial Domain Adaptation in Binary Classification with Complex and Noisy Data.\n\n\n\n\n\n\nSeptember 1, 2019\n\n\nDylan Spicker and Melissa Van Bussel\n\n\n\n\n\n\n  \n\n\n\n\nMeasurement Error in Precision Medicine and Dynamic Treatment Regimes.\n\n\n\n\n\nTalk given at the Statistical Society of Canada general meeting (2019, in Calgary). (PDF of Slides)\n\n\n\n\n\n\nMay 27, 2019\n\n\nDylan Spicker\n\n\n\n\n\n\n  \n\n\n\n\nOGS Queen Elizabeth II Graduate Scholarship in Science and Technology (QEII-GSST).\n\n\n\n\n\n$15,000/year; 1 year. Received a competitive, 1 year award through OGS.\n\n\n\n\n\n\nMay 1, 2019\n\n\nDylan Spicker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "thoughts/teaching-philosophy-statement/index.html",
    "href": "thoughts/teaching-philosophy-statement/index.html",
    "title": "My Teaching Philosophy Statement",
    "section": "",
    "text": "Back to 'My Thoughts'\n\n\n\nSomewhere between weighing the 600th and 700th snack-sized bag of chips I thought to myself “why?” Having seen the difficulty that students have understanding sampling variability, population distributions, and sampling distributions, I formed the most visual, memorable example lesson I could: randomly select bags of chips from a box and weigh them. Then repeat this process 15 times to estimate the mean and further repeat this mean estimation 50 times for the sampling distribution. Judging by student results, and the messages from students I get to this day, the resulting tutorial was both effective and memorable.\nSo why weigh 750 bags of chips for a 20 minute tutorial? Because I care deeply. I care about statistics education, which I view as critical to fostering an informed citizenry. I care about my students as learners and individuals, and I care about their diversity of background and experience (where they exhibit far more variability than those bags of chips). The following pillars demonstrate how I translate this care into an actionable plan for effective teaching, (beyond relying on sampling snack-sized bags of chips).\n\nAutonomous, Personalised Learning and Differentiated Instruction: Students enter a course with diverse goals and interests, and, as a result, tailoring course materials increases student engagement and improves outcomes. I differentiate content and instructional strategies by using personal surveys to determine student interests and preferred learning styles, and use this information to cater my lectures accordingly (e.g., using both sports data and COVID-19 trial data for example problems). I strive to ensure that there is a path for every student in the class, that the students with insufficient background knowledge are not left behind and the thriving students are properly challenged. Additionally, I give my students as much choice as possible on assessments, allowing them to select between proof-based or application-based questions on assignments, and between formats and topics for final projects.\nSafe and Welcoming Learning Environment: I create an environment that ensures students are able to be fully present, absent additional burden. I build my courses so as to remove stressors whenever possible, for instance, using flexible grading schemes. I have deep personal experience with modern issues related to inclusivity and diversity, and I proactively highlight the importance of these issues in my course materials and interactions with students. This fosters an environment of mutual respect and allows all students to be fully present as themselves. I encourage frequent communication, course related or not, and I am available to students before and after lectures, for in-person or virtual office hours, through email, instant messaging, course forums, and with anonymous surveys.\nUniversal Accessibility by Design: My goal is to remove any possible disadvantages for students which could arise based on the design of the course. I use a wide array of accessible content types in my courses, including closed-captioned lecture videos, lecture notes, internal and external supplementary material, annotated and unannotated lecture slides, and example code. Doing so ensures that every learner has easy-to-access, suitable content for their abilities and learning styles.\nMistakes Matter: Mistakes, when carefully considered by students, are a crucial part of the learning process. Students are generally, and understandably, hesitant to make mistakes on important assessments, and this view is often carried over to lower-stakes settings. I encourage students to view mistakes as opportunities to reflect and learn, and I reinforce this with course policies. For instance, when feasible, I allow my students to earn back partial credit by re-submitting their assignments after correcting their mistakes, with a small explanation of what their fundamental misunderstanding was.\nOrganisation of All Course Aspects: Particularly with my focus on personalised learning, the administration, content, and expectations of a course must be organised and cohesive, clearly defining what is necessary for success. For instance, I divide my lecture videos not weekly, but based on topic. For each topic, I release a conceptual video, followed by videos on the theory and application of the concepts, allowing for easy revisiting and learning without mode switching. I provide a suggested viewing schedule and clearly state which videos are necessary for success on each assignment."
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html",
    "href": "thoughts/controlling-for-factors/index.html",
    "title": "When Should We Control For Factors?",
    "section": "",
    "text": "Back to 'My Thoughts'"
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html#simpsons-paradox",
    "href": "thoughts/controlling-for-factors/index.html#simpsons-paradox",
    "title": "When Should We Control For Factors?",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nThere is a well known statistical “paradox” Simpson’s Paradox, which discusses the ability for a trend observed in your data to reverse when groups of interest are aggregated. The basic idea is often illustrated with the UC Berkeley Gender Bias example. If you took a look at the admissions data for 1973, you would note that 44% of men who applied, and only 35% of women, were accepted into their respective programs. However, four of the six departments at the school, women were accepted at higher rates than men. The issue is that, on the whole, women were applying to more competitive departments (such as English), where mentended to apply to less competitive disciplines (Engineering and Chemistry). This creates a scenario where there is no explicit discrimination at any department level, but it appears as though there is when aggregated.\nThe lesson from the above, statistically, tends to be condition on relevant quantities. That is, when we are conducting statistical analysis, we ought to “control for” the effects of possibly influential variables; if the above analysis had initially looked at acceptance rates controlling for department of application, a more correct conclusion would have been reached. Seems easy enough - so what is the issue?"
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html#berksons-paradox",
    "href": "thoughts/controlling-for-factors/index.html#berksons-paradox",
    "title": "When Should We Control For Factors?",
    "section": "Berkson’s Paradox",
    "text": "Berkson’s Paradox\n(Prepare for Deja Vu) There is a well known statistical “paradox” Berkson’s Paradox, which discusses the ability for a correlation observed in your data to reverse based on the set of individuals observed. The basic idea is often illustrated with Why Are Handsome Men Such Jerks? example. If you imagine scoring the handsomeness, and the niceness, of men numerically, then perhaps Jim will only date a guy who has some niceness plus handsomeness score that exceeds a threshold. We may say that, in the general population, nicer men also tend to be more handsome (who knows?) so that these two traits are positively correlated. However, among the men that Jim dates, he will observe that nicer men are on average less handsome (and vice-versa). Why? Well, if there is an exceedingly nice man, in order to cross the dating threshold for Jim, this guy does not need to be very handsome at all - and as such, there will be more observations that are extreme than would be expected in the general public.\nThe lesson from the above, statistically, tends to be do not condition on irrelevant quantities. That is, when we are conducting statistical analysis, we ought not “control for” the effects of irrelevant variables; if the above analysis had inititally looked at the pool of men that Jim dates, then we would erroneously conclude that handsomeness and niceness are negatively correlated."
  },
  {
    "objectID": "thoughts/controlling-for-factors/index.html#well-now-what",
    "href": "thoughts/controlling-for-factors/index.html#well-now-what",
    "title": "When Should We Control For Factors?",
    "section": "Well, now what?",
    "text": "Well, now what?\nWithout having a subject-matter understanding of the causal structure at play, it is not obvious as to whether a factor is a confounder (e.g. in the first example) or a collider (e.g. in the second example). Confounders should always be conditioned on in a causal analysis, where colliders should never be conditiond on. Doing this incorrectly will lead to incorrect conclusions regarding the presence of a causal effect. This ultimately means that we should not “control for” absolutely every, possible factor; it also ultimately means that we ought to “control for” every relevant factor. Because this cannot be empirically informed, causal inference necessitates importing assumptions from a subject matter expert."
  },
  {
    "objectID": "thoughts/linearity-of-q-functions/index.html",
    "href": "thoughts/linearity-of-q-functions/index.html",
    "title": "On the Linearity of Q-Functions",
    "section": "",
    "text": "Back to 'My Thoughts'\n\n\n\nA problem of particular focus for the past few days for me has been on attempting to establish when we can make the claim of linearity in the set of Q-functions, based on an outcome model and the relevant distributions of covariates which go into it.\nBriefly, if we define a DTR over \\(K\\) stages, which we wish to fit using Q-learning, we know that the parameter estimates for the Q-functions will be consistent, so long as all of the models are correctly specified. Since each Q-function takes the form \\(Q_k = E[V_{k+1}|H_k]\\), then it is natural to wish to use linear regression models for these functions as they each take the form of a conditional expectation.\nHowever, the quantities \\(V_{k}\\) are given based on the maximization of the previous Q-function (where here, previous actually refers to the \\(k+1\\)-st function), over the treatment parameter, which is binary. As such, these value functions will contain an indicator function times by some linear functional, which of course has the potential to induce severe non-linearity into the Q-functions (particularly as they propagate over time).\nThis problem is more formally discussed in (P. J. Schulte, A. A. Tsiatis, E. B. Laber, and M. Davidian, “Q-and a-learning methods for estimating optimal dynamic treatment regimes,” Statistical Science, vol. 29, no. 4, pp. 640–661,2014.), where they demonstrate that even in a simple scenario, this non-linearity is a problem.\nSo… what? There are two obvious questions that arise from the revelation that Q-functions may, in fact, be non-linear. (1) Are there more flexible models that we can use in place of linear regression? (2) Can we characterize precisely when Q-functions will be linear?\nFor (1) the answer is “yes” and it is done somewhat frequently. This is, of course, a more fruitful avenue of research moving forward, and will certainly be where I turn my attention next. The second question, however, is quite interesting to me.\nIt is certainly possible (in fact, upon some moderate thought, not all that difficult) to define DTRs which do indeed have linear Q-functions; I am interested in seeing whether sufficient (and, perhaps more aspirationally, necessary) conditions are possible."
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html",
    "href": "thoughts/PSA-causation-without-correlation/index.html",
    "title": "PSA: Causation without Correlation",
    "section": "",
    "text": "Back to 'My Thoughts'"
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#correlation-can-imply-causation",
    "href": "thoughts/PSA-causation-without-correlation/index.html#correlation-can-imply-causation",
    "title": "PSA: Causation without Correlation",
    "section": "Correlation can Imply Causation",
    "text": "Correlation can Imply Causation\nThere are situations where merely observing correlation between two quantities does in fact mean that those two quantities are causally linked. There is an entire field of Statistics dedicated to this distinction. The most common method for detecting causation from correlation is in randomized studies. In a properly conducted, well-powered randomized trial, we can guarantee (with some set degree of confidence) that correlations observed are causal in nature. Of course, as with all findings in Statistics, we can never be certain of the causal nature, but with enough well-collected data, we can be as certain as we need to be.\nEven without randomization, we are able to detect causal relations from observed correlations. These so-called observational studies tend to be more difficult to use for this purpose (e.g. they require stronger assumptions, more data, a sound theoretical backing, or all of the above), they are still frequently and reliably used for this purpose. Consider the case of smoking and lung cancer. It would be entirely unethical to randomly assign groups of individuals (at birth) to be either smokers or non-smokers, and then follow them and collect information regarding the rates of lung-cancer. Instead, we rely on observational data to detect this causal relationship. We have seen enough times (in enough settings) to know that this is a causal relationship. There is a strong scientific backing explaining this phenomenon, and it has reliably held-up over time.\nSo while correlation does not imply causation, it can imply causation, given we take into account enough relevant factors. A Statistics 102 student recognizes this fact, and will often then tell you “correlation does not imply causation, but causation does imply correlation”. Unfortunately, unlike the Statistics 101 student from before, this student is actually simply incorrect. Causation does not imply correlation!"
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#causation-does-not-imply-correlation",
    "href": "thoughts/PSA-causation-without-correlation/index.html#causation-does-not-imply-correlation",
    "title": "PSA: Causation without Correlation",
    "section": "Causation does NOT Imply Correlation",
    "text": "Causation does NOT Imply Correlation\nThis is counter-intuitive (but true). The reason here has to do with the definition of correlation. When people say correlation, they almost certainly mean Pearson’s Correlation, as this is the most commonly used measure of correlation. Pearson’s Correlation is a measure of the linear relationship between two quantities. This is very useful in many settings, but it only takes a moment thought to recognize that non-linear relationships exist. In fact, the following plot would exhibit a correlation of 0.\n\n\n\nLucid Mesh: Non-linear Data\n\n\nNow the two quantities pictured have a very clear relationship, it just happens to be non-linear. As such the correlation is 0. Is the relationship between these two quantities causal? Who knows! But the point is that the observation that correlation equals 0 is a point which gives literally no information as to whether or not a causal relationship exists [I mean, technically it does tell you that it is not possible for it to be a strictly, unmediated linear relationship, but that’s so little information as to basically be no information]."
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#a-set-of-concrete-examples",
    "href": "thoughts/PSA-causation-without-correlation/index.html#a-set-of-concrete-examples",
    "title": "PSA: Causation without Correlation",
    "section": "A Set of Concrete Examples",
    "text": "A Set of Concrete Examples\nTo illustrate the above, we are going to use a set of examples with some real math (don’t worry - it’s not too difficult). These will give you an anchoring point to recognize the difficulty in trying to have a succint statement to capture the interplay between correlation and causation.\n\nTake \\(Y\\) to be some outcome - we will simply call it “utility” so that it is hard to argue against my proposed models.\nTake \\(X\\) to be some measurement about a person - we will simply call it “success” because, again, these are my models! We will assume that \\(X\\) comes before \\(Y\\) in time, so that it could be a cause of \\(Y\\).\nTake \\(S\\) to be a variable representing the sex of an individual, which will take the value of \\(1\\) to represent females and -1 otherwise.\n\n\nCorrelation Implying Causation\nIf we propose that \\(Y = 2X\\) then \\(X\\) and \\(Y\\) have a correlation of \\(1\\). Now, we assume that we have measured everything about these individuals, and this is the only relationship that exists. In this setting, \\(X\\) also directly causes \\(Y\\). That is, an individuals utility is exactly equal to twice their success. Since success comes before utility, success can be said to cause utility.\n\n\nCorrelation without Causation\nNow, let’s say that we observe the exact same model \\(Y = 2X\\), where the correlation is still \\(1\\). Now, we also make the following observations:\n\nFirst, males have a utility of \\(-50\\) and everyone else has a utility of \\(50\\)\nSecond, males have a success of \\(-25\\) and everyone else has a success of \\(25\\)\n\nIn this setting, the exact causal mechanism may not be as clear. However, given the above two observations it seems reasonable to propose that \\(X\\) is caused by \\(S\\), through the model \\(X = 25S\\). Similary, it seems reasonable to propose the model that \\(Y = 50S\\). These two models combine to suggest \\(Y = 2X\\). However, in this setting we have a confounding relationship.\nTo clearly state what is happening: Sex causes utility, and sex causes success. There is no causal relationship between utility and sucess, however, because of the confounder sex it appears as though there is one.\n\n\nCausation without Correlation\nFinally, we propose the model \\(Y = 2AX\\). That is, the utility observed is two times the success of a female, and it is negative two times the success of a male - more successful females have higher utility, more successful males have lower utility.\nIf we assume that there is no relationship between sex and outcome, and we assume that there is an equal probability of being male or female, then we will find that the correlation between utility and success is exactly 0. However, success still causes the utility, it just acts differently on males or females. If you tell me an individuals sex and their success, I can tell you their utility."
  },
  {
    "objectID": "thoughts/PSA-causation-without-correlation/index.html#conclusions",
    "href": "thoughts/PSA-causation-without-correlation/index.html#conclusions",
    "title": "PSA: Causation without Correlation",
    "section": "Conclusions",
    "text": "Conclusions\nSo, what can we say? Well, correlation does not imply causation, except sometimes it does. Further, causation does not imply correlation, unless the relationship is strictly linear. It is certainly less catchy, but also far more accurate, than the standard, so I will understand if it doesn’t catch on!"
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "",
    "text": "Back to 'My Thoughts'\nThis term I had the pleasure of teaching an introductory statistics course at the University of Waterloo. In preparing to run the course I came to realize (and this is by no means a unique realization) that there is a lot of incorrect (or misleading) information aimed at people learning statistics. One particularly stand-out example is the idea that we can use the mean and median of a sample to infer skewness.\nNow, all told this is not the worst offense of this kind, but I think it exemplifies a concerning trend. There is an understandable desire for easy to follow rules in statistics. It is my opinion that, generally speaking, such rules do not exist. This same tendency leads to an uncritical acceptance of arbitrary significance levels and inappropriate applications of approximation theorems. So, in the interest of the bigger picture, let’s correct this misconception!\nThis point has previously been made (von Hippel 2005), but we can compile some explicit examples here."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Positive Skew; Mean < Median",
    "text": "Positive Skew; Mean &lt; Median\nConsider responses to the question ``How many adults (18+) live in this house?’’ for residents across Canada. We can imagine asking 1000 households for this response and it seems plausible that we would have data which look something like:\n\n\n\n\n\nBased on these data, we can see fairly clearly that the median is going to be 2 and that the data exhibit a positive skewness. The mean, however, is given by 1.79 which evidently violates the supposed rule of thumb."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-median-mean",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-median-mean",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Negative Skew; Median < Mean",
    "text": "Negative Skew; Median &lt; Mean\nConsider a quiz that is given to students in an introductory statistics class that is marked out of 5. If we assume that 500 students are enrolled in the course, and they all write the quiz, then the following grade distribution seems plausible:\n\n\n\n\n\nBased on these data, we can see fairly clearly that the median is going to be 4 and that the data exhibit a negative skewness. The mean, however, is given by 4.1 which evidently violates the supposed rule of thumb."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median-1",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#positive-skew-mean-median-1",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Positive Skew; Mean = Median",
    "text": "Positive Skew; Mean = Median\nDropping the pretense of real data we can, as may be obvious now, begin to generate arbitrary examples that violate the remaining configuartions of the rules. The following data are selected so that the skew is positive (0.6491939), while the mean (0) and the median (0) are exactly equal.\n\n\n\n\n\nPerhaps the interested reader could come up with an example situation for these data."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-mean-median",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#negative-skew-mean-median",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "Negative Skew; Mean = Median",
    "text": "Negative Skew; Mean = Median\nThe previous example can of course be mirrored exactly so as to produce a negative sample skewness (-0.6491939), while the mean (1) and the median (1) are exactly equal.\n\n\n\n\n\nPerhaps the interested reader could come up with an (alternative) example situation for these data."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mean-median-mode",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mean-median-mode",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "No Skew; Mean < Median (< Mode)",
    "text": "No Skew; Mean &lt; Median (&lt; Mode)\nThe remaining scenarios to deconstruct involve distributions which have no skew, but which have a mean and median which are not equal. These are fairly straightforward to construct as well. In this example we can see that the skewness (0) is 0, while the mean (4.5) and the median (5) differ."
  },
  {
    "objectID": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mode-median-mean",
    "href": "thoughts/2021-04-20-the-mean-median-and-skewness/index.html#no-skew-mode-median-mean",
    "title": "The (lack of a) Relationship between Mean, Median, and Skewness",
    "section": "No Skew; (Mode <) Median < Mean",
    "text": "No Skew; (Mode &lt;) Median &lt; Mean\nOnce again, other alternatives are possible, by why reinvent the wheel when we can simply mirror our previous situation? In this example we can see that the skewness (0) is 0, while the mean (5.5) and the median (5) still differ, but differently than last time."
  },
  {
    "objectID": "thoughts/null-preservation/index.html",
    "href": "thoughts/null-preservation/index.html",
    "title": "Null Preservation",
    "section": "",
    "text": "Back to 'My Thoughts'\n\n\n\nI am in the process of reading Causal Inference by Hernan and Robins, and one idea has come up a handful of times which strikes me as important - and underdiscussed in a standard statistical curriculum (read: my statistical curriculum).\nThe basic premise is that, under null preservation, the model that we are working with is never misspecified. So instance, if one is considering the null of no causal influece of treatment (\\(a\\)) on an outcome (\\(Y\\)), then the standard quadratic regression \\[E[Y|A=a] = \\beta_0 + \\beta_1a + \\beta_2a^2\\] is correctly specified when there is no causal effect [e.g. it is some constant] even if the mean structure between Y and A is incorrect (supposing it exists).\nThis is a neat concept in part because it means that a conclusion in which we reject the null is valid evidence against the null, regardless of whether or model is correctly specified. If we take a conservative statisticians point-of-view, and never claim evidence in favour of the alternative, then null preservation permits us to draw valid conclusions, even when model mis-specification is all but certain."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html",
    "href": "thoughts/research-philosophy-statement/index.html",
    "title": "My Research Philosophy Statement",
    "section": "",
    "text": "Back to 'My Thoughts'\nQuestions of causality sit at the heart of scientific inquiry and progress. While historically causality has been inferred through the use of randomised trials and experiments, the rate of growth of modern observational data and the corresponding complexity necessitates the development of novel causal techniques in order to ensure continued scientific progress. I am a causal inference researcher with an interest in developing novel methods to accommodate the growing complexity of modern data. I focus on developing techniques which are rigorously justified while remaining widely accessible. I am strongly committed to interdisciplinary research, which helps to ensure that my independent research remains grounded and readily applicable. I have a record of productive, impactful research, capable of attracting competitive funding. My interests are in burgeoning areas, tackling modern statistical issues, with diverse applied utility, and strongly integrated interdisciplinary partnerships. This provides access to a wide variety of funding sources, and serves as a strong foundation for a publishable, fundable, independent research program."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#past-research",
    "href": "thoughts/research-philosophy-statement/index.html#past-research",
    "title": "My Research Philosophy Statement",
    "section": "Past Research",
    "text": "Past Research\nMy past work focused on developing methods which account for the effects of measurement error and misclassification in the context of causal inference with dynamic treatment regimes (DTRs). DTRs provide a framework for capturing causal inference in a longitudinal setting, often applied for precision medicine, where we estimate the optimal sequence of treatments for individual patients. Measurement error is a pervasive issue, particularly in biostatistics, occurring whenever a variate of interest cannot be accurately observed. I provided the first account of the impacts of measurement error on optimal DTR estimation, and developed the first set of estimators to correct for these issues [1]. Additionally, I highlighted the shortcomings of ignoring patient nonadherence in optimal DTR estimation, and provided a locally efficient, computationally feasible estimation technique for the valid estimation of causal effects in these settings [2]. This work has resulted in a manuscript published in Statistics in Medicine, a top-tier Biostatistics journal, with a secondary article under consideration, has received competitive NSERC grant funding, and has since spawned additional contributions in the field [3, 4]. Beyond my work in modern causal inference, I have developed general measurement error corrections, leveraging both nonparametric [5] and robust [6] statistics, producing two additional manuscripts which are currently under review."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#current-research",
    "href": "thoughts/research-philosophy-statement/index.html#current-research",
    "title": "My Research Philosophy Statement",
    "section": "Current Research",
    "text": "Current Research\nI am continuing to expand methods for optimal DTR estimation with complex data, where I have begun work related to patient privacy in DTR estimation. Differential privacy is a standard which ensures that reported estimates do not reveal too much information regarding any individual in a study, and preserving this privacy is critically important for statistical analyses in precision medicine. I am also working on techniques for sample size and power calculations within the DTR framework, an important area missing from the current literature. Outside of work on DTRs, my postdoctoral research focuses on the analysis of data from respondent-driven sampling (RDS). RDS combines benefits from probability based sampling and snowball sampling, providing an effective technique for researchers studying hard-to-reach populations. It has correspondingly seen wide uptake by applied researchers. Despite its frequent application, existing techniques for RDS analysis focus primarily on mean and proportion estimation, in a cross-sectional setting. I am developing a set of causal estimators using data arising from RDS, techniques which are essential for answering scientific questions with RDS data. Additionally, I am extending both the causal estimators and the existing mean estimators to a longitudinal RDS setting. This work is currently funded with a competitive postdoctoral grant, issued by CANSSI, and is motivated through a partnership with the Engage study. Engage is an ongoing, multicenter RDS, operating out of Montreal, Toronto, and Vancouver, aiming to “inform HIV and STBBI prevention techniques in the population of gay, bi, and queer men, including trans men, and other men who have sex with men.”"
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#future-research",
    "href": "thoughts/research-philosophy-statement/index.html#future-research",
    "title": "My Research Philosophy Statement",
    "section": "Future Research",
    "text": "Future Research\nGoing forward, I will continue to develop novel methods in causal inference for modern issues in real-world data. In the short-term I will study complex outcomes in the context of DTRs. In the DTR literature, it is common to assume that there is a single outcome to be optimised for all individuals. In practice there are often multiple, competing outcomes. A medical treatment may be thought to increase life expectancy, but this needs to be balanced against potential negative side effects. Existing work on competing outcomes in DTRs ignores the multivariate nature of the problem, opting instead for simplified combinations using utility functions. This assumes that utility is estimable or else can be constructed, and that future patients will balance their priorities similarly. My proposed strategy will provide more flexibility for combinations of competing outcomes, compared with univariate utility functions. DTRs are typically studied under the assumption that there is a known, finite number of decisions to make. There is some work considering infinite time horizon DTRs through the use of Markovian assumptions. While these assumptions are mathematically convenient, they can be oversimplified in real-world scenarios, particularly in medical settings where delayed treatment effects are common. I will consider a joint modelling approach, treating the number of decisions as a random quantity in addition to the outcomes of interest. This joint modelling approach will borrow from standard longitudinal and survival analysis, and will provide a more realistic set of assumptions for real-world decision making. In the longer term, I will develop tools for causal inference on complex, modern data types. For instance, I will develop causal methodologies for both network and functional data. Both network and functional data are becoming more prominent, and each represents an important form of “big data”. Alongside the associational techniques used to study these large data sets, causal methods will be required to answer scientific questions which arise. This is particularly important as observational, rather than experimental, data are driving the increase in the availability of network and functional data. Causal network analysis has received some attention in the literature, but has remained relatively simple both in terms of associational structures and causal estimands (methods tend to assume only local interference patterns on networks in a way which often is not realistic). Functional data, on the other hand, have received comparatively little causal attention, despite the potential applied utility (for instance, with fMRI data)."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#existing-and-planned-collaborative-research",
    "href": "thoughts/research-philosophy-statement/index.html#existing-and-planned-collaborative-research",
    "title": "My Research Philosophy Statement",
    "section": "Existing and Planned Collaborative Research",
    "text": "Existing and Planned Collaborative Research\nBeyond my impactful methodological developments, I have a strong focus on conducting interdisciplinary research. The purpose of this is two-fold: first, working with applied researchers helps to discern important methodological needs, ensuring that my independent research remains relevant. Second, the primary purpose of statistical methodology is for the advancement of scientific knowledge. Working on interdisciplinary research facilitates the theoretically valid application of cutting-edge statistical techniques, enhancing the pursuit of scientific knowledge. An example of this can be seen during my doctoral studies, where I established an ongoing research project with nutritional scientists, computer scientists, and statisticians. Our research aims to study the impacts of diet on health outcomes, while taking into account the complexities of dietary patterns (including combination effects which are historically ignored), using deep learning techniques. This ongoing project involves statistical methodology, algorithmic development, and nutritional science. In addition, during my postdoctoral studies, my work on RDS is directly informed by a close working relationship with medical and public health researchers from the aforementioned Engage study. This partnership ensures that the methodological developments are necessary, and will see applied use. Moving forward, I intend to continue initiating and participating in relevant interdisciplinary work, particularly in the health sciences. This provides exciting avenues for collaboration, a wider array of funding sources, and helps to ensure uptake of methodological developments. Several of my short-term projects provide opportunities for interdisciplinary research in personalised medicine, public health, and medical imaging. In addition to health applications, the possibility of industry collaboration exists, particularly related to functional and network data. Functional data is commonly observed in finance and speech recognition settings, while network data naturally arises in social networks and logistics tasks."
  },
  {
    "objectID": "thoughts/research-philosophy-statement/index.html#references",
    "href": "thoughts/research-philosophy-statement/index.html#references",
    "title": "My Research Philosophy Statement",
    "section": "References",
    "text": "References\n[1] Spicker, Dylan, and Wallace, Michael P. Measurement error and precision medicine: Error-prone tailoring covariates in dynamic treatment regimes. Statistics in Medicine. 2020; 39: 3732– 3755. https://doi.org/10.1002/sim.8690\n[2] Spicker , Dylan. Generalizations to Corrections of Measurement Error Effects for Dynamic Treatment Regimes. UWSpace. 2022. http://hdl.handle.net/10012/18581.\n[3] Wallace, Michael. P. Measurement error and precision medicine. In T. Cai, B. Chakraborty, E. Laber, E. Moodie, and M. van der Laan, editors, Handbook of Statistical Methods for Precision Medicine [Accepted], Handbooks of Modern Statistical Methods. Chapman and Hall/CRC, Boca Raton Florida. 2022\n[4] Liu, Dan. Regression-based Methods for Dynamic Treatment Regimes with Mismeasured Covariates or Misclassified Response. 2022.\n[5] Spicker, Dylan, Wallace, Michael P., and Yi, Grace Y. Nonparametric Simulation Extrapolation for Measurement Error Models. arXiv preprint arXiv:2111.02863. 2021.\n[6] Spicker, Dylan, Wallace, Michael P., and Yi, Grace Y. Generalizations to Corrections for the Effects of Measurement Error in Approximately Consistent Methodologies. arXiv preprint arXiv:2106.07401. 2021."
  },
  {
    "objectID": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html",
    "href": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html",
    "title": "Notes on ‘von Mises Calculus for Statistical Functionals’",
    "section": "",
    "text": "Back to 'My Thoughts'\nDisclaimer: These notes are my (attempted, possibly error-prone) summary of Fernholz (1983)."
  },
  {
    "objectID": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#why-might-we-care",
    "href": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#why-might-we-care",
    "title": "Notes on ‘von Mises Calculus for Statistical Functionals’",
    "section": "Why might we care?",
    "text": "Why might we care?\nThe primary reason that we have taken the time to derive the von Mises derivative, influence curve, and so forth is that we can use it for a Taylor-esque expansion of functionals. We write the von Mises expansion of \\(T\\) as \\[T(G) = T(F) + T_F'(G-F) + \\text{Rem}(G-F),\\] where \\(\\text{Rem}(G-F)\\) captures a remainder term.\nIf we take \\(G = F_n\\), then we get \\[\\begin{align*}\n  T(F_n) &= T(F) + T_F'(F_n - F) + \\text{Rem}(F_n - F) \\\\\n  &= T(F) + \\int\\phi_{F}(x)d(F_n - F)(x) + \\text{Rem}(F_n - F) \\\\\n  &= T(F) + \\int\\phi_F(x)dF_n(x) - 0 + \\text{Rem}(F_n - F) \\\\\n  &= T(F) + \\frac{1}{n}\\sum_{i = 1}^n \\phi_F(X_i) + \\text{Rem}(F_n - F).\n\\end{align*}\\]\nNow, asymptotic inference tends to center on the quantity \\(\\widehat{\\theta} - \\theta\\), or in our functional notation, \\(T(F_n) - T(F)\\). Assuming that we have a well behaved influence curve we could apply central limit theorems to \\(\\sqrt{n}(T(F_n) - T(F))\\) to get nice convergence results, so long as \\(\\sqrt{n}\\text{Rem}(F_n - F)\\) converges in probability to \\(0\\). The idea is then to use these expansions to investigate the asymptotic properties.\nThe von Mises derivative, unfortunately, is not sufficient for continuation of this study. It is too weak of a concept to be able to generally conclude that, when it exists, \\(\\sqrt{n}\\text{Rem}(F_n - F) \\stackrel{p}{\\to} 0\\). To overcome this, it is possible to assume that the functional is twice differentiable (with respect to the von Mises derivative), which would enable such conclusions. This is an overly restrictive assumption, and so instead we turn to alternative formulations of the functional derivative.\n\nThis example demonstrates the shortcoming of the von Mises derivative. Consider the functional \\(T(F)\\) that measures the size of the discontinuity points of \\(F\\) on the interval \\([0,1]\\). Take \\(\\alpha &gt; 1\\), then we can write \\[T(F) = \\sum_{x\\in[0,1]} \\left(F(x) - F(x^{-})\\right)^{\\alpha},\\] where \\(F(x^{-}) = \\lim\\limits_{x^*\\to x} F(x^*)\\). On the interval \\([0,1]\\) any distribution function has at most a countable number of discontinuities, so this sum only takes values at a countable number of values, and is as such well-defined (if it helps, define \\(\\mathcal{D}(F)\\) to be the set of discontinuities of \\(F\\) on \\([0,1]\\), and then define the summation for \\(x \\in \\mathcal{D}(F)\\), or similar).\nNow, if we continue \\(F = U\\) to be the distribution function of a \\(\\text{Uniform}(0,1)\\) distribution, then it is clear that \\(T(U) = 0\\). Moreover, the empirical distribution function, computed based on a \\(\\text{Uniform}(0,1)\\) distribution, will almost surely have \\(n\\) discontinuities, each of size \\(\\frac{1}{n}\\). As a result, we will find that \\(T(F_n) = n\\times\\frac{1}{n^\\alpha} = n^{1-\\alpha}\\).\nResultingly, we have that \\(\\sqrt{n}(T(F_n) - T(U)) \\stackrel{a.s}{=} n^{3/2 - \\alpha}\\). However, we can also see that the von Mises derivative exists (and is exactly \\(0\\), so long as \\(\\alpha &gt; 1\\)), which means that for any \\(\\alpha \\in (1, 3/2)\\) we must not have \\(\\text{Rem}(F_n - F) = o_p(n^{-1/2})\\)."
  },
  {
    "objectID": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#a-topological-digression",
    "href": "thoughts/2021-04-27-von-mises-calculus-notes-I/index.html#a-topological-digression",
    "title": "Notes on ‘von Mises Calculus for Statistical Functionals’",
    "section": "A Topological Digression",
    "text": "A Topological Digression\n\nA topological vector space, given by \\((V(\\mathbb{K}), \\mathcal{T})\\), is a vector space \\(V(\\mathbb{K})\\) on a field \\(\\mathbb{K}\\) (either \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)), along with a topology \\(\\mathcal{T}\\), such that addition and scalar multiplication is jointly continuous in both variables. Note, the somewhat unusual notation of \\(V(\\mathbb{K})\\) is just used to stress the field that the vector space is defined on: this will be dropped shortly since I really only care about \\(\\mathbb{K} = \\mathbb{R}\\).\n\nWe introduce the concept of a topological vector space so to build to the concept of the weak and weak\\(*\\) topologies. The idea is to extend the idea of a normed vector space (e.g. \\((V, ||\\cdot||_V)\\)) which can evidently generate its own topology (the norm topology, by taking open balls based on \\(||\\cdot||_V\\)) to allow for weaker (smaller in the set containment sense) topologies. The rationale is that convergence concepts are tied directly (for many convergence notions) to topologies. I will leave this here for now and revist topological notions of convergence (which seems to involve nets – gross).\n\nThe dual space, \\(V^*\\) of a toplogical vector space, \\((V(\\mathbb{K}), \\mathcal{T})\\) is the space of continuous linear functionals from \\(V(\\mathbb{K})\\to\\mathbb{K}\\).\n\nThe dual is continuously brought up in the considerations of functional analysis. There are (apparently) good reasons as to why this is the case. From my perspective, this definition is here mainly as a reminder: for a space \\(V\\) the dual is the space of continuous linear functions from \\(V\\) to the field on which \\(V\\) is defined (the more I say/type it, hopefully the sooner it sticks!).\n\nWe take \\(V\\) to be a topological vector space and \\(V^*\\) to be its dual. Then, the weak topology on \\(V\\) is the topology generated by the seminorms, \\(|\\lambda(x)|\\) for all \\(\\lambda \\in V^*\\). The weak\\(*\\) topology is the topology generated by the seminorms \\(|\\lambda(x)|\\) for all \\(x \\in V\\)."
  },
  {
    "objectID": "courses/STAT437/course_index.html",
    "href": "courses/STAT437/course_index.html",
    "title": "STAT 437",
    "section": "",
    "text": "Course Outline - PDF Version\nSTAT 437 - Lectures Overview"
  },
  {
    "objectID": "courses/STAT437/course_index.html#administrative-information",
    "href": "courses/STAT437/course_index.html#administrative-information",
    "title": "STAT 437",
    "section": "",
    "text": "Course Outline - PDF Version\nSTAT 437 - Lectures Overview"
  },
  {
    "objectID": "courses/STAT437/course_index.html#assignments-and-solutions",
    "href": "courses/STAT437/course_index.html#assignments-and-solutions",
    "title": "STAT 437",
    "section": "Assignments and Solutions",
    "text": "Assignments and Solutions\n\nAssignment 1 (Solution)\nAssignment 2 (Solution)\nAssignment 3 (Solution)\nMidterm Test (Solution)\nPaper Review Assignment\nFinal Project"
  },
  {
    "objectID": "courses/STAT437/course_index.html#helper-code-and-supplementary-notes",
    "href": "courses/STAT437/course_index.html#helper-code-and-supplementary-notes",
    "title": "STAT 437",
    "section": "Helper Code and Supplementary Notes",
    "text": "Helper Code and Supplementary Notes\n\nQuasi-Likelihood Theory in Full (Supplementary Notes)\ndata_import_helper.R\nhelper_functions.R"
  },
  {
    "objectID": "courses/STAT437/course_index.html#lecture-videos-slides-notes-and-code",
    "href": "courses/STAT437/course_index.html#lecture-videos-slides-notes-and-code",
    "title": "STAT 437",
    "section": "Lecture Videos, Slides, Notes, and Code",
    "text": "Lecture Videos, Slides, Notes, and Code\n\n(Lecture 001) Welcome to STAT 437\n(Lecture 002) What are longitudinal data?\n\nLecture Slides\n\n(Lecture 003) Exploring Longitudinal Data (Application)\n\nLecture Code\n\n(Lecture 004) Notation for Longitudinal Data (Theory)\n\nLecture Notes\n\n(Lecture 005) What is Linear Regression (Review; Theory)\n\nLecture Slides\n\n(Lecture 006) Continuous Longitudinal Data: Why Can’t we Just Use Regression? (Linear Marginal Models)\n\nLecture Slides\n\n(Lecture 007) Linear Marginal Models: Likelihood, Inference, and Asymptotics (Theory)\n\nLecture Notes\n\n(Lecture 008) Linear Marginal Models: Implementation in R (Application)\n\nLecture Code\n\n(Lecture 009) What are generalized linear models? (Review; Theory)\n\nLecture Slides\n\n(Lecture 010) Marginal Models: Accommodating non-continuous outcomes\n\nLecture Slides\n\n(Lecture 011) M-Estimation: A Practicing Statistician’s Best Friend (Conceptual, Theory, and Application)\n\nLecture Slides\nLecture Notes\n\n(Lecture 012) Generalized Estimating Equations: Estimating parameters from Marginal Models\n\nLecture Slides\n\n(Lecture 013) Generalized Estimating Equations: Examples of GEEs (Theory)\n\nLecture Notes\n\n(Lecture 014) Generalized Estimating Equations: Details of Asymptotic Inference (Theory)\n(Lecture 015) Generalized Estimating Equations: COVID-19 Example\n\nLecture Code\n\n(Lecture 016) Generalized Estimating Equations: Epilepsy Trial Example\n\nLecture Code\n\n(Lecture 017) From the Population to the Individual: Mixed Effects Models\n\nLecture Slides\n\n(Lecture 018) Linear Mixed Effects Models\n\nLecture Slides\n\n(Lecture 019) Linear Mixed Effects Models (Theory)\n\nLecture Notes\n\n(Lecture 020) Variance Testing Considerations: Constrained LRT\n(Lecture 021) Linear Mixed Effects Models (Application)\n\nLecture Code\n\n(Lecture 022) Transition Models for Longitudinal Data\n\nLecture Slides\n\n(Lecture 023) Transition Models (Theory)\n\nLecture Notes\n\n(Lecture 024) Transition Models (Application)\n\nLecture Code\n\n(Lecture 025) Handling Missing Data in Longitudinal Models\n\nLecture Notes\nLecture Slides\n\n(Lecture 026) Handling Missing Data in Longitudinal Models - MCAR, NMAR, and Likelihood Techniques\n\nLecture Code\n\n(Lecture 027) Handling Missing Data in Longitudinal Models - Imputation and Weighting\n\nLecture Code\n\n(Lecture 028) Recap of Longitudinal Methods\n\nLecture Slides\n\n(Lecture 029) Introduction to Time-to-Event Data\n\nLecture Slides\n\n(Lecture 030) Quantities of Interest for Survival Analysis\n\nLecture Notes\n\n(Lecture 031) Discrete Time to Event Data\n\nLecture Slides\n\n(Lecture 032) Discrete Time to Event Data (Theory)\n\nLecture Notes\n\n(Lecture 033) Discrete Time to Event Data Exploration (Application)\n\nLecture Code\n\n(Lecture 034) Logistic Regression and Proportional Odds Models\n\nLecture Slides\n\n(Lecture 035) Logistic Regression and Proportional Odds Models (Application)\n\nLecture Code\n\n(Lecture 036) Introduction to Continuous Time Survival Analysis\n\nLecture Slides\n\n(Lecture 037) Continuous Time Survival Analysis, Likelihood Construction (Theory)\n\nLecture Notes\n\n(Lecture 038) Location Scale Family Distributions, with log-linear Regression (Theory)\n(Lecture 039) Continuous Time Regression Models using Survreg (Application)\n\nLecture Code\n\n(Lecture 040) Accelerated Failure Time Models\n\nLecture Slides\n\n(Lecture 041) Accelerated Failure Time Models (Theory)\n\nLecture Notes\n\n(Lecture 042) Accelerated Failure Time Models (Application)\n\nLecture Code\n\n(Lecture 043) Proportional Hazards Models\n\nLecture Slides\n\n(Lecture 044) Proportional Hazards Models (Theory)\n(Lecture 045) Proportional Hazards Models (Application)\n\nLecture Code"
  },
  {
    "objectID": "courses/STAT437/course_index.html#data-files",
    "href": "courses/STAT437/course_index.html#data-files",
    "title": "STAT 437",
    "section": "Data Files",
    "text": "Data Files\nTo save these data files, right-click and save as. The data are presented for educational purposes only. Sources for the data are available in the data_import_helper.R script, and data should be used only to follow along with the relevant lectures. Any use of this data beyond these purposes needs to be cleared with the data owners."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching (and Course Notes)",
    "section": "",
    "text": "STAT 437\n\n\nStatistical Methods for Life History Analysis\n\n\n\n\n\n\nUniversity of Waterloo\n\n\nWinter 2022\n\n\n\n\n\n\n  \n\n\n\n\nSTAT 231\n\n\nStatistics\n\n\n\n\n\n\nUniversity of Waterloo\n\n\nWinter 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Thoughts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMy Research Philosophy Statement\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\njob search\n\n\n\n\nThis document outlines my research experience, approach, and goals for the future.\n\n\n\n\n\n\nDec 14, 2022\n\n\nDylan Spicker\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nMy Teaching Philosophy Statement\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\njob search\n\n\n\n\nThis document outlines my teaching experience, approach, and goals for the future.\n\n\n\n\n\n\nDec 14, 2022\n\n\nDylan Spicker\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘von Mises Calculus for Statistical Functionals’\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nI am attempting to work through Luisa Fernholz’s excellent ‘von Mises Calculus for Statistical Functionals’ – these are my notes, pulling in the required concepts from elsewhere..\n\n\n\n\n\n\nApr 27, 2021\n\n\nDylan Spicker\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nNull Preservation\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nThe idea of null preservation is an interesting, and under-discussed, concept.\n\n\n\n\n\n\nJul 16, 2019\n\n\nDylan Spicker\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nOn the Linearity of Q-Functions\n\n\n\n\n\n\n\nstatistics\n\n\nresearch\n\n\n\n\nThis blog post explores when, if ever, the Q-functions (from DTRs) can be linear.\n\n\n\n\n\n\nNov 15, 2019\n\n\nDylan Spicker\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPSA: Causation without Correlation\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nThis document investigates the all-too-familiar claim: ‘Correlation is not causation’.\n\n\n\n\n\n\nMay 26, 2019\n\n\nDylan Spicker\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nThe (lack of a) Relationship between Mean, Median, and Skewness\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nmisconceptions\n\n\n\n\nThere is a commonly taught ‘rule-of-thumb’ that states that we can determine the skewness of a distribution based on the relative location of the mean and the median. This is not true.\n\n\n\n\n\n\nApr 20, 2021\n\n\nDylan Spicker\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nWhen Should We Control For Factors?\n\n\n\n\n\n\n\nstatistics\n\n\nteaching\n\n\nresearch\n\n\n\n\nWe are often told to control for confounding factors, but when should we?\n\n\n\n\n\n\nJul 1, 2019\n\n\nDylan Spicker\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  }
]